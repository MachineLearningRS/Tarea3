{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div><img src=\"escudo_utfsm.gif\" style=\"float:right;height:100px\">\n",
    "<img src=\"IsotipoDIisocolor.png\" style=\"float:left;height:100px\">\n",
    "<center>\n",
    "    <h1> INF-493 - Machine Learning</h1>\n",
    "    <h1> Tarea 2 - Métodos para Clasificación </h1>\n",
    "\n",
    "<p>\n",
    "<br><center>_Javier Reyes_<strong> - </strong>_javier.reyes.12@sansano.usm.cl_<strong> - </strong>_201273524-6_ </center>\n",
    "<br><center>_Marco Salinas_<strong> - </strong>_marco.salinas.12@sansano.usm.cl_<strong> - </strong>_201273589-0_ </center>\n",
    "</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1 Small Circle inside Large Circle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)** Escriba una función que genere (aleatoriamente) $n$ datos etiquetados de la forma $\\{(x_1; y_1), ... ,(x_n; y_n)\\}, x_i \\in R^2, y_i \\in \\{0, 1\\}$, con una distribución de probabilidad que re eje la configuración linealmente inseparable que muestra la Fig. 1. Utilice esta función para crear 1000 datos de entrenamiento y 1000 datos de pruebas. Para medir la tendencia de los modelos a sobre-ajuste, agregue un 5% de ruido al dataset, generando x's cercanos a la frontera. Genere un gráfico que muestre datos de entrenamiento y pruebas, identificando cada clase con un color diferente.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def do_circles(n=2000,noisy_n=0.05):\n",
    "    generator = check_random_state(10)\n",
    "    linspace = np.linspace(0, 2 * np.pi, n // 2 + 1)[:-1]\n",
    "    outer_circ_x = np.cos(linspace)\n",
    "    outer_circ_y = np.sin(linspace)\n",
    "    inner_circ_x = outer_circ_x * .3\n",
    "    inner_circ_y = outer_circ_y * .3\n",
    "    X = np.vstack((np.append(outer_circ_x, inner_circ_x),\n",
    "                   np.append(outer_circ_y, inner_circ_y))).T\n",
    "    \n",
    "    y = np.hstack([np.zeros(n // 2, dtype=np.intp),\n",
    "                   np.ones(n // 2, dtype=np.intp)])\n",
    "    X += generator.normal(scale=noisy_n, size=X.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "X_train,Y_train,X_test,Y_test = do_circles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    Para lo que sigue de la actividad utilice la siguiente función para graficar las fronteras de clasificación en\n",
    "base a la probabilidad, definida por un algoritmo, de un ejemplo a pertenecer a una clase en particular.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "def plot_classifier(clf,X_train,Y_train,X_test,Y_test,model_type,lim):\n",
    "    f, axis = plt.subplots(1, 1, sharex='col', sharey='row', figsize=(12, 8))\n",
    "    axis.scatter(X_train[:,0],X_train[:,1],s=30,c=Y_train,zorder=10,cmap=plt.cm.Set3,label=\"train data\")\n",
    "    axis.scatter(X_test[:,0],X_test[:,1],s=20,c=Y_test,zorder=10,cmap='tab20b')\n",
    "    XX, YY = np.mgrid[-2:2:200j, -2:2:200j]\n",
    "    if model_type == 'tree':\n",
    "        Z = clf.predict_proba(np.c_[XX.ravel(), YY.ravel()])[:,0]\n",
    "        axis.set_title(\"Decision Tree Classifier\")\n",
    "    elif model_type != 'tree':\n",
    "        Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "        axis.set_title(model_type)\n",
    "    else: raise ValueError('model type not supported')\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    Zplot = Z >= 0.5\n",
    "    axis.pcolormesh(XX, YY, Zplot ,cmap='Greys')\n",
    "    axis.contour(XX, YY, Z, alpha=1, colors=[\"k\", \"k\", \"k\"], linestyles=[\"--\", \"-\", \"--\"], levels=[-2, 0, 2])\n",
    "    navy_train = mpatches.Patch(color='navy',label='Train data')\n",
    "    lightblue_train = mpatches.Patch(color='lightblue',label='Train data')\n",
    "    yellow_test = mpatches.Patch(color='yellow',label='Test data')\n",
    "    violet_test = mpatches.Patch(color='violet',label='Test data')\n",
    "    axis.legend(handles=[navy_train,lightblue_train,yellow_test,violet_test])\n",
    "    axis.set_xlim(-lim,lim)\n",
    "    axis.set_ylim(-lim,lim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Tree = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "Tree.fit(X_train, Y_train)\n",
    "plot_classifier(Tree,X_train,Y_train,X_test,Y_test,\"tree\",1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C=100, penalty='l2', tol=0.01)\n",
    "LR.fit(X_train, Y_train)\n",
    "plot_classifier(LR,X_train,Y_train,X_test,Y_test,\"Logistic Regression\",1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM\n",
    "model_SVM = SVM(kernel='rbf')\n",
    "model_SVM.fit(X_train,Y_train)\n",
    "plot_classifier(model_SVM,X_train,Y_train,X_test,Y_test,\"SVM\",1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)** Demuestre __experimentalmente__ que una red neuronal artificial correspondiente a 1 sola neurona (i.e. sin capas escondidas) no puede resolver satisfactoriamente el problema. Puede utilizar la función de activación y el método de entrenamiento que prefiera. Sea convincente: por ejemplo, intente modificar los parámetros de la máquina de aprendizaje, reportando métricas que permitan evaluar el desempeño del modelo en el problema con cada cambio efectuado. Adapte también la función plot classifier para que represente gráficamente la solución encontrada por la red neuronal. Describa y explique lo que observa, reportando gráficos de la solución sólo para algunos casos representativos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "n_h=1\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=X_train.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(n_h, init='uniform', activation='sigmoid'))\n",
    "model.compile(optimizer=SGD(lr=1), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, Y_train, epochs=50, batch_size=100, verbose=1)\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "test_acc = scores[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    Se puede observar que el conjuto de datos 1 esta separado de una manera uniforme, pero en uno de sus extremos se mezclan con los datos del conjunto 2, haciendo imposible una separación para discriminar los datos con el algoritmo de LDA (a través de una recta). La única manera de poder utilizar el clasificador, es eliminando los outlayers, aunque esto es muy riesgoso porque pueden ser información valiosa.\n",
    "</p>\n",
    "    \n",
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)** Demuestre __experimentalmente__ que una red neuronal artificial con 1 capa escondida puede resolver satisfactoriamente el problema obtenido en (a). Puede utilizar la arquitectura y el método de entrenamiento que prefiera, pero en esta actividad puede optar tranquilamente por usar los hiper-parámetros que se entregan como referencia en el código de ejemplo. Cambie el número de neuronas Nh en la red entre 2 y 32 en potencias de 2, graficando el error de entrenamiento y pruebas como función de Nh. Describa y explique lo que observa. Utilice la función *plot_classifier*, diseñada anteriormente, para construir gráficos de la solución en algunos casos representativos.\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_h=32\n",
    "model = Sequential()\n",
    "model.add(Dense(n_h, input_dim=X_train.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    asdf\n",
    "</p>\n",
    "\n",
    "<p  style=\"text-align: justify;\">\n",
    "    asdf\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)** Demuestre **experimentalmente** que stump (árbol de clasificación de 1 nivel) no puede resolver satisfactoriamente el problema anterior. Puede utilizar el criterio y la función de partición que prefiera. Sea convincente: por ejemplo, intente modificar los parámetros de la máquina, reportando métricas que permitan evaluar el desempeño del modelo en el problema con cada cambio efectuado. Adapte también la función *plot_classifier* para que represente gráficamente la solución encontrada por el árbol. Describa y explique lo que observa, reportando gráficos de la solución sólo para algunos casos representativos.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "clf=Tree(criterion='gini',splitter='best',random_state=0,max_depth=1)\n",
    "clf.fit(X_train,Y_train)\n",
    "acc_test = clf.score(X_test,Y_test)\n",
    "print(\"Test Accuracy = %f\"%acc_test)\n",
    "print(clf.tree_.max_depth)\n",
    "plot_classifier(clf,X_train,Y_train,X_test,Y_test,'tree',1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)** Demuestre experimentalmente que un árbol de clasificación de múltiples niveles puede resolver satisfactoriamente el problema estudiado. Puede utilizar el criterio y la función de partición que prefiera, pero puede optar tranquilamente por usar los hiper-parámetros que se entregan como referencia en el código de ejemplo. Cambie el número de niveles admitidos en el árbol Nt entre 2 y 20, graficando el error de entrenamiento y pruebas como función de Nt. Describa y explique lo que observa. Utilice la función plot classifier, dise~nada anteriormente, para construir gráficos de la solución en algunos casos representativos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t=19\n",
    "clf=Tree(criterion='gini',splitter='best',random_state=0,max_depth=n_t)\n",
    "clf.fit(X_train,Y_train)\n",
    "plot_classifier(clf,X_train,Y_train,X_test,Y_test,\"tree\",1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def DTC_plot(X_train,Y_train,X_test,Y_test):\n",
    "    \n",
    "    n_ts = range(2,21)\n",
    "    mse_DTC_train = []\n",
    "    mse_DTC_test = []\n",
    "\n",
    "    for n_t in n_ts:\n",
    "        clf=Tree(criterion='gini',splitter='best',random_state=0,max_depth=n_t)\n",
    "        clf.fit(X_train, Y_train)\n",
    "\n",
    "        y_pred_DTC_train = clf.predict(X_train)\n",
    "        y_pred_DTC_test = clf.predict(X_test)\n",
    "\n",
    "        mse_DTC_train.append(1-accuracy_score(Y_train, y_pred_DTC_train))\n",
    "        mse_DTC_test.append(1-accuracy_score(Y_test, y_pred_DTC_test))\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    DTC_plot = plt.subplot(1,1,1)\n",
    "    DTC_plot.plot(n_ts, mse_DTC_train, label='Training Misclassification Error')\n",
    "    DTC_plot.plot(n_ts, mse_DTC_test, label='Testing Misclassification Error')\n",
    "    plt.legend(bbox_to_anchor=(1,0.6))\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.xlabel('Allows Levels')\n",
    "    plt.title(\"DTC\")\n",
    "    plt.show()\n",
    "    \n",
    "DTC_plot(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)** Como ya se demostró experimentalmente que este problema es linealmente inseperable, ahora se pide experimentar otra alternativa. Para ello deberá realizar una proyección de los datos a un nuevo espacio dimensional (manifold) en el cual se reconozcan sus patrones no lineales, para poder trabajarlos con fronteras lineales. Utilice la técnica de PCA con la ayuda de un Kernel Gaussiano  para extraer sus vectores con dimensión infinita de mayor varianza.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=2,kernel=\"rbf\", gamma=5)\n",
    "kpca = kpca.fit(X_train)\n",
    "Xkpca_train = kpca.transform(X_train)\n",
    "Xkpca_test = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(g)** Ajuste un algoritmo de aprendizaje con fronteras lineal para los datos proyectados en este nuevo espacio que captura sus componentes no lineales, muestre graficamente que el problema ahora puede ser resulto con estos métodos. Reporte métricas para evaluar el desempeño, comente y concluya.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "Tree.fit(Xkpca_train, Y_train)\n",
    "plot_classifier(Tree,Xkpca_train,Y_train,Xkpca_test,Y_test,\"DTC\",0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(C=100, penalty='l2', tol=0.01)\n",
    "LR.fit(Xkpca_train, Y_train)\n",
    "plot_classifier(LR,Xkpca_train,Y_train,Xkpca_test,Y_test,\"LR\",0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVM = SVM(kernel='linear')\n",
    "model_SVM.fit(Xkpca_train,Y_train)\n",
    "plot_classifier(model_SVM,Xkpca_train,Y_train,Xkpca_test,Y_test,\"SVM\",0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Bike Sharing: Predicción de Demanda Horaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    En esta parte de la experencia se trabajará con datos de audios los cuales son directamente extraídos desde datos fuentes _.wav_, lo que corresponde a una señal de sonido en diferentes tiempos. El _dataset_ se denomina _ Heartbeat Sounds_, el cual consta de grabaciones de sonidos de latidos cardíacos normales y anormales, con distintas categorías para los latidos anormales. Los datos fueron obtenidos desde un publico general a través de la aplicación de iPhone iStethoscope Pro.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "def clean_filename(fname, string):\n",
    "    file_name = fname.split('/')[1]\n",
    "    if file_name[:2] == '__':\n",
    "        file_name = string + file_name\n",
    "    return file_name\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "def load_wav_file(name, path):\n",
    "    s, b = wavfile.read(path + name)\n",
    "    assert s == SAMPLE_RATE\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)** Construya un dataframe con los datos a analizar. Describa el dataset y determine cuántos registros hay\n",
    "por clase.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./heartbeat-sounds/set_a.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    El _dataset_ posee 176 datos, los cuales poseen 4 columnas cada uno.\n",
    "    <ul style=\"list-style-type:disc;margin-left:24px\">\n",
    "        <li>**dataset:** columna que indica a que dataset corresponde el dato</li>\n",
    "        <li>**fname:** corresponde al nombre del archivo de audio</li>\n",
    "        <li>**label:** puede ser \"normal\", estar en blanco (para datos sin etiqueta), y anormales donde pueden ser \"artifac\", \"extrahls\" y \"murmur\". </li>\n",
    "        <li>**sublabel:** columna sin datos.</li>\n",
    "\n",
    "    </ul>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p  style=\"text-align: justify;\">\n",
    "    Descripcion de las categorias:\n",
    "    <ul style=\"list-style-type:disc;margin-left:24px;text-align: justify;\">\n",
    "        <li>**Normal:** Corresponden a sonidos de corazones normales y saludables.</li>\n",
    "        <li>**Murmur (Soplo):** Corresponden a soplos cardíacos, que suenan como si hubiera un ruido de \"silbido, rugido, estruendo o fluido turbulento\". </li>\n",
    "        <li>**Extra Heart Sound:** Correspoden a una medición en la cual se escucha un sonido adicional del corazón.  </li>\n",
    "        <li>**Artifact:** En esta categoria se encuentra una amplia gama de sonidos diferentes, que incluyen chillidos y ecos de realimentación, voz, música y ruido. Gerenalmente no hay sonidos cardíacos discernibles. Esta categoría es la más diferente de las otras. </li>\n",
    "        <li>**Unlabelled:** Son aquellas muestras que no estan etiquetadas en ninguna categoria. </li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "|     Categoria     \t| Registro \t|\n",
    "|:-----------------:\t|:--------:\t|\n",
    "|       Normal      \t|    31    \t|\n",
    "|       Murmur      \t|    34    \t|\n",
    "| Extra Heart Sound \t|    19    \t|\n",
    "|      Artifact     \t|    40    \t|\n",
    "|     Unlabelled    \t|    52    \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['dataset','sublabel',],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Artifact\",df.label.value_counts()['artifact'])\n",
    "print(\"Murmur\", df.label.value_counts()['murmur'])\n",
    "print(\"Extra Heart Sound\", df.label.value_counts()['extrahls'])\n",
    "print(\"Normal\", df.label.value_counts()['normal'])\n",
    "print(\"Unlabelled\", 176-40-34-19-31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)** Lea los archivos _.wav_ y transformelos en secuencias de tiempo. Realice un _padding_ de ceros al final de\n",
    "cada secuencia para que todas queden representadas con la misma cantidad de elementos, explique la\n",
    "importancia de realizar este paso.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padd_zeros(array,length):\n",
    "    aux = np.zeros(length)\n",
    "    aux[:array.shape[0]] = array\n",
    "    return aux\n",
    "\n",
    "\n",
    "new_df =pd.DataFrame({'file_name' : df['fname'].apply(clean_filename,string='Aunlabelledtest')})\n",
    "new_df['time_series'] = new_df['file_name'].apply(load_wav_file, path='./heartbeat-sounds/set_a/')\n",
    "new_df['len_series'] = new_df['time_series'].apply(len)\n",
    "new_df['time_series'] = new_df['time_series'].apply(padd_zeros,length=max(new_df['len_series']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    La importancia de realizar este paso, es normalizar la dimensionalidad de cada vector de datos, para así trabajar con todos en la misma dimensión.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)** Manipule los datos y cambie las etiquetas de los audios por otras asignadas por un doctor experto,\n",
    "el cual afirma que estos cambios son requeridos. Vuelva a determinar cuántos registros hay por clase.\n",
    "Nótese que ahora son 3 clases ¿Explique la problemática de tener etiquetas mal asignadas en los datos?\n",
    "¿Un solo dato puede afectar esto?\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "             1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
    "             2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
    "             1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0,\n",
    "             2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0,\n",
    "             0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "\n",
    "labels = ['artifact','normal/extrahls', 'murmur']\n",
    "new_df['target'] = [labels[i] for i in new_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Artifact\",new_df.target.value_counts()['artifact'])\n",
    "print(\"Murmur\", new_df.target.value_counts()['murmur'])\n",
    "print(\"Normal and EHSound\", new_df.target.value_counts()['normal/extrahls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          Categoria         \t| Registro \t|\n",
    "|:--------------------------:\t|:--------:\t|\n",
    "| Normal / Extra Heart Sound \t|    65    \t|\n",
    "|           Murmur           \t|    53    \t|\n",
    "|          Artifact          \t|    58    \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   Como ahora las etiquetas son 3, se particionaron la clasificación de datos anteriormente reasignandolos a las nuevas etiquetas. Esto puede provocar un problema en cuanto a la nueva clasificación, ya que las nuevas etiquetas pueden que no representen en su totalidad al dato asignado y quede mal asignado por haber particionado el set de clasificación anterior.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "   Un solo dato puede afectar el porcentaje de acierto que anteriormente se obtuvo con los 2 tipos de clasificaciones, porque como se explicó anteriormente, las nuevas clases podrían no representar en su totalidad al dato al momento de clasificarlo, aunque también, puede que se clasifique de mejor manera el set de datos. Un claro ejemplo es ver la asignación de los datos en una nueva Split de un árbol de decisión.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)**  Codifique las distintas clases a valores numéricos para que puedan ser trabajados por los algoritmos\n",
    "clasificadores.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df[\"target\"] = new_df[\"target\"].astype('category')\n",
    "cat_columns = new_df.select_dtypes(['category']).columns\n",
    "new_df[cat_columns] = new_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)** Desordene los datos, evitando así el orden en el que vienen la gran mayoría de las etiquetas. Cree la\n",
    "matriz que conforma a los datos en sus dimensiones sin preprocesar, es decir, cada ejemplo es una\n",
    "secuencia de amplitudes en el tiempo. ¿Las dimensiones de ésta indica que puede generar problemas?\n",
    "¿De qué tipo?\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = new_df.sample(frac=1,random_state=44)\n",
    "X = np.stack(new_df['time_series'].values, axis=0)\n",
    "y = new_df.target.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   Puede generar problemas de computo y tiempo. Al agregar dimensionalidad a un problema, trae como beneficio un mayor porcentaje de acierto, porque hay una mayor cantidad de atributos con los cuales clasificar de manera correcta los datos, pero la desventaja es que mientras de más dimensión sea el conjunto de datos, mayor tiempo tomará en procesar la data y clasificarla, por lo que en general, no es recomendable aumentar la dimensionalidad.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)** Para pre procesar la secuencia en el tiempo realice una Transformada de Fourier discreta para pasar\n",
    "los datos desde el dominio de tiempos al dominio de frecuencias presentes en la señal de sonido.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fourier = np.abs(np.fft.fft(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(g)**  Para seguir con el pre procesamiento realice un muestreo representativo de los datos a través de una\n",
    "técnica de muestreo especializada en secuencias ¿En qué beneficia este paso? ¿Cómo podría determinar\n",
    "si el muestro es representativo?\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "X_resampled = []\n",
    "for i in range(X_fourier.shape[0]):\n",
    "    sequence = X_fourier[i,:].copy()\n",
    "    resampled_sequence = signal.resample(sequence, 100000)\n",
    "    X_resampled.append(resampled_sequence)\n",
    "\n",
    "X_resampled = np.array(X_resampled)\n",
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   Este paso benificia en el tiempo computacional que tomará clasificar los datos, ya que reduce la dimensionalidad a su cuarta parte arpoximadamente.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "   Habría que ver todos los atributos y ver qué representan cada uno de ellos, para ver si en el proceso de mezclarlos y obtener nuevos atributos y conseguir una menor dimensionalidad, no se perdieron atributos representativos dentro del conjunto de atributos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(h)**  Genere un conjunto de pruebas mediante la técnica _hold-out validation_ para verificar la calidad de los\n",
    "clasificadores. ¿Cuántas clases tiene y de qué tamaño queda cada conjunto?\n",
    "\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=43)\n",
    "print(\"El set de entrenamiento tiene \" + str(len(X_train)) \n",
    "      + \" y el set de test tiene \" + str(len(X_test)) \n",
    "      + \" datos, y ambos tienen 10000 clases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(i)**  Realice un proceso de estandarizar los datos para ser trabajados adecuadamente. Recuerde que solo se\n",
    "debe ajustar (calcular media y desviación estándar) con el conjunto de entrenamiento.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(j)**  Realice una reducción de dimensionalidad a través de la técnica PCA, para representar los datos en $d = 2$ dimensiones. Recuerde que solo se debe ajustar (encontrar las componentes principales) con el conjunto de entrenamiento. Visualice apropiadamente la proyección en 2 dimensiones.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "d=2\n",
    "pca_model = PCA(n_components=d)\n",
    "pca_model.fit(X_train)\n",
    "X_pca_train = pca_model.transform(X_train)\n",
    "X_pca_test = pca_model.transform(X_test)\n",
    "plot(X_pca_train,y_train)\n",
    "plot(X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(k)**  Entrene un modelo de Regresión Logística variando el parámetro de regularización $C$ construyendo un gráfico resumen del error en función de este hiper-parámetro. Además entrene una Máquina de Soporte Vectorial (SVM) con kernel lineal, variando el hiper-parámetro de regularizacion $C$ en el mismo rango que para la Regresión Logística, construyendo el mismo gráfico resumen. Compare.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def LR_SVM(X_train,y_train,X_test,y_test):\n",
    "    mse_LR_train = []\n",
    "    mse_LR_test = []\n",
    "\n",
    "    mse_SVM_train = []\n",
    "    mse_SVM_test = []\n",
    "\n",
    "    Ks = []\n",
    "    Ks_ticks = [0.0001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "    for i, Cs in enumerate((0.0001,0.01,0.1,1,10,100,1000)):\n",
    "\n",
    "        model_LR = LR(C=Cs)\n",
    "        model_SVM = SVM(C=Cs)\n",
    "\n",
    "        model_LR.fit(X_train, y_train)\n",
    "        model_SVM.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_LR_train = model_LR.predict(X_train)\n",
    "        y_pred_LR_test = model_LR.predict(X_test)\n",
    "\n",
    "        y_pred_SVM_train = model_SVM.predict(X_train)\n",
    "        y_pred_SVM_test = model_SVM.predict(X_test)\n",
    "\n",
    "        mse_LR_train.append(1-accuracy_score(y_train, y_pred_LR_train))\n",
    "        mse_LR_test.append(1-accuracy_score(y_test, y_pred_LR_test))\n",
    "\n",
    "        mse_SVM_train.append(1-accuracy_score(y_train, y_pred_SVM_train))\n",
    "        mse_SVM_test.append(1-accuracy_score(y_test, y_pred_SVM_test))\n",
    "\n",
    "        Ks.append(i+1)\n",
    "\n",
    "        #print(\"Miss Classification Loss for LR: %f\"%(1-accuracy_score(y_train, y_pred_LR)))\n",
    "        #print(\"Miss Classification Loss for SVM: %f\"%(1-accuracy_score(y_train, y_pred_SVM)))\n",
    "\n",
    "        #print(\"C=%.5f\" % Cs)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    LR_plot = plt.subplot(2,1,1)\n",
    "    LR_plot.plot(Ks, mse_LR_train, label='LR Training Misclassification Error')\n",
    "    LR_plot.plot(Ks, mse_LR_test, label='LR Testing Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.xlabel('Parámetro Regularizador $C$')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.axis([1,6,0.0, 0.7])\n",
    "    plt.xticks(Ks, Ks_ticks)\n",
    "\n",
    "    SVM_plot = plt.subplot(2,1,2)    \n",
    "    SVM_plot.plot(Ks, mse_SVM_train, label = 'SVM Training Misclassification Error')\n",
    "    SVM_plot.plot(Ks, mse_SVM_test, label = 'SVM Test Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.xlabel('Parámetro Regularizador $C$')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.axis([1,6,0.0, 0.7])\n",
    "    plt.xticks(Ks, Ks_ticks)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.xlabel('Parametro Regularización $C$')\n",
    "    #plt.ylabel('Miss Classification Error')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "LR_SVM(X_pca_train,y_train,X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "Se puede apreciar a través de los gráficos que el método de Regesión Logística empieza con un menor error de entrenamiento y de test cuando el parámetro de regularización $C$ es $0.0001$ en comparación con la SVM, pero a medida de que $C$ aumenta, el parámetro de regularización $C$ regulariza de mejor manera los datos de entrenamiento a través de SVM que de LR, pero regulariza de mejor menera el conjunto de test en LR que en SVM, cuando  $C \\rightarrow \\infty$. El menor error alcanzado por LR es aproximadamente de $0.4$ en ambos conjuntos de datos, es más, sí $C \\rightarrow \\infty$, ambos errores convergen a $0.4$. Por otro lado, en SVM el menor error en el conjunto de test converge a 0 sí  $C \\rightarrow \\infty$, en cambio con el conjunto de test, su menor error es aproximadamente de $0.45$ con $C = 1$. <br>\n",
    "Luego de este análisis, se puede afirmar que para este conjunto de datos de entrenamiento, es mejor usar SVM con un parámetro de regularización $C$ grande para obtener el menor error posible y utilizar LR con un parámetro de regularización $C$ grande para obtener el menor error posible sobre el conjunto de datos de test.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(l)**  Entrene un Arbol de Decisión, con la configuración que estime conveniente, variando el hiper-parámetro regularizador _max depth_, construyendo un gráfico resumen del error en función de este parámetro.\n",
    "Compare con los modelos anteriores.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "def DTC_plot(X_pca_train,y_train,X_pca_test,y_test):\n",
    "\n",
    "    Depths = range(1,30)\n",
    "\n",
    "    mse_DTC_train = []\n",
    "    mse_DTC_test = []\n",
    "\n",
    "    for i in Depths:\n",
    "        model_DTC = DTC(max_depth = i)\n",
    "\n",
    "        model_DTC.fit(X_pca_train, y_train)\n",
    "\n",
    "        y_pred_DTC_train = model_DTC.predict(X_pca_train)\n",
    "        y_pred_DTC_test = model_DTC.predict(X_pca_test)\n",
    "\n",
    "        mse_DTC_train.append(1-accuracy_score(y_train, y_pred_DTC_train))\n",
    "        mse_DTC_test.append(1-accuracy_score(y_test, y_pred_DTC_test))\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    DTC_plot = plt.subplot(1,1,1)\n",
    "    DTC_plot.plot(Depths, mse_DTC_train, label='Training Misclassification Error')\n",
    "    DTC_plot.plot(Depths, mse_DTC_test, label='Testing Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.xlabel('Depth DTC')\n",
    "    plt.axis([0,30,0.0, 0.7])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "DTC_plot(X_pca_train,y_train,X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    En comparación con los resultados anteriores, es posible ver que LR sigue manteniendo superioridad sobre el conjuntos de datos de test, ya que obtiene el menor error entre los 3 métodos. En el caso del conjunto de entrenamiento, el DTC converge a 0 con mayor rápidez que la SVM con un parámetro regularizador $C$ grande. Además, se tiene la ventaja que el DTC es más simple de implementar y gastas menos recursos que la SVM (dependiendo de la profundidad del árbol).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(m)**  Experimente con diferentes dimensiones $d$ para la proyección de PCA con el propósito de obtener un\n",
    "modelo con menor error. Construya una tabla o gráfico resumen.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def plots_MCE_PCA(D, C_LR, C_SVM):\n",
    "    mse_LR_train = []\n",
    "    mse_LR_test = []\n",
    "    mse_SVM_train = []\n",
    "    mse_SVM_test = []\n",
    "\n",
    "    model_LR = LR(C=1000)\n",
    "    model_SVM = SVM(C=1)\n",
    "\n",
    "    dim = np.arange(1,D)\n",
    "\n",
    "    for d in range(1,D):\n",
    "\n",
    "        pca_model = PCA(n_components=d)\n",
    "        pca_model.fit(X_train)\n",
    "\n",
    "        X_pca_train = pca_model.transform(X_train)\n",
    "        X_pca_test = pca_model.transform(X_test)\n",
    "\n",
    "        model_LR.fit(X_pca_train, y_train)\n",
    "        model_SVM.fit(X_pca_train, y_train)\n",
    "\n",
    "        y_pred_LR_train = model_LR.predict(X_pca_train)\n",
    "        y_pred_LR_test = model_LR.predict(X_pca_test)\n",
    "\n",
    "        y_pred_SVM_train = model_SVM.predict(X_pca_train)\n",
    "        y_pred_SVM_test = model_SVM.predict(X_pca_test)\n",
    "\n",
    "        mse_LR_train.append(1-accuracy_score(y_train, y_pred_LR_train))\n",
    "        mse_LR_test.append(1-accuracy_score(y_test, y_pred_LR_test))\n",
    "        mse_SVM_train.append(1-accuracy_score(y_train, y_pred_SVM_train))\n",
    "        mse_SVM_test.append(1-accuracy_score(y_test, y_pred_SVM_test))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    LR_plot = plt.subplot(2,1,1)\n",
    "    LR_plot.plot(dim, mse_LR_train, label='LR Training Misclassification Error')\n",
    "    LR_plot.plot(dim, mse_LR_test, label='LR Testing Misclassification Error')\n",
    "    plt.legend(loc='center right')\n",
    "    plt.xlabel('Dimensiones')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "\n",
    "    SVM_plot = plt.subplot(2,1,2)    \n",
    "    SVM_plot.plot(dim, mse_SVM_train, label = 'SVM Training Misclassification Error')\n",
    "    SVM_plot.plot(dim, mse_SVM_test, label = 'SVM Test Misclassification Error')\n",
    "    plt.legend(loc='center right')\n",
    "    plt.xlabel('Dimensiones')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.xlabel('Parametro Regularización $C$')\n",
    "    #plt.ylabel('Miss Classification Error')\n",
    "\n",
    "    plt.show()  \n",
    "\n",
    "widgets.interact(plots_MCE_PCA,D = (1,20),C_LR = (1,1000,1),C_SVM=(1,1000,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(n)**  Realice otra reducción de dimensionalidad ahora a través de la técnica LDA, para representar los datos en $d = 2$ dimensiones. Recuerde que sólo se debe ajustar con el conjunto de entrenamiento, si se muestra un warning explique el porqué. Visualice apropiadamente la proyección en $2$ dimensiones.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "model_lda = LDA(n_components=2)\n",
    "model_lda.fit(X_train,y_train)\n",
    "X_lda_train = model_lda.transform(X_train)\n",
    "X_lda_test = model_lda.transform(X_test)\n",
    "\n",
    "plot(X_lda_train, y_train)\n",
    "#plot(X_pca_test, y_test)\n",
    "\n",
    "#y_pred_lda = model_lda.predict(X_pca_train)\n",
    "\n",
    "#visualize_border(model_lda, X_train, y_train,\"LDA\")\n",
    "#print(X_pca_train)\n",
    "#https://stats.stackexchange.com/questions/29385/collinear-variables-in-multiclass-lda-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   Aparece un warning para advertir que hay variables o atributos colineales, es decir, que son linealmente dependientes.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(o)**  Con el propósito de encontrar el mejor modelo vuelva a realizar el item k) con el l) en el nuevo espacio generado por la representación según las d dimensiones de la proyección LDA. Esta nueva representación ¿mejora o empeora el desempeño? Explique.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "\n",
    "model_lda = LDA(n_components=2)\n",
    "model_lda.fit(X_train,y_train)\n",
    "X_lda_train = model_lda.transform(X_train)\n",
    "X_lda_test = model_lda.transform(X_test)\n",
    "\n",
    "print(X_lda_train.shape)\n",
    "print(X_lda_test.shape)\n",
    "\n",
    "def LR_SVM_DTC(X_train,y_train,X_test,y_test):\n",
    "\n",
    "    mse_LR_train = []\n",
    "    mse_LR_test = []\n",
    "\n",
    "    mse_SVM_train = []\n",
    "    mse_SVM_test = []\n",
    "\n",
    "    Ks = []\n",
    "    Ks_ticks = [0.0001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "    for i, Cs in enumerate((0.0001,0.01,0.1,1,10,100,1000)):\n",
    "\n",
    "        model_LR = LR(C=Cs)\n",
    "        model_SVM = SVM(C=Cs)\n",
    "\n",
    "        model_LR.fit(X_train, y_train)\n",
    "        model_SVM.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_LR_train = model_LR.predict(X_train)\n",
    "        y_pred_LR_test = model_LR.predict(X_test)\n",
    "\n",
    "        y_pred_SVM_train = model_SVM.predict(X_train)\n",
    "        y_pred_SVM_test = model_SVM.predict(X_test)\n",
    "\n",
    "        mse_LR_train.append(1-accuracy_score(y_train, y_pred_LR_train))\n",
    "        mse_LR_test.append(1-accuracy_score(y_test, y_pred_LR_test))\n",
    "\n",
    "        mse_SVM_train.append(1-accuracy_score(y_train, y_pred_SVM_train))\n",
    "        mse_SVM_test.append(1-accuracy_score(y_test, y_pred_SVM_test))\n",
    "\n",
    "        Ks.append(i+1)\n",
    "\n",
    "        #print(\"Miss Classification Loss for LR: %f\"%(1-accuracy_score(y_train, y_pred_LR)))\n",
    "        #print(\"Miss Classification Loss for SVM: %f\"%(1-accuracy_score(y_train, y_pred_SVM)))\n",
    "\n",
    "        #print(\"C=%.5f\" % Cs)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    LR_plot = plt.subplot(2,1,1)\n",
    "    LR_plot.plot(Ks, mse_LR_train, label='LR Training Misclassification Error')\n",
    "    LR_plot.plot(Ks, mse_LR_test, label='LR Testing Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.xlabel('Parámetro Regularizador $C$')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.axis([1,6,0.0, 0.7])\n",
    "    plt.xticks(Ks, Ks_ticks)\n",
    "\n",
    "    SVM_plot = plt.subplot(2,1,2)    \n",
    "    SVM_plot.plot(Ks, mse_SVM_train, label = 'SVM Training Misclassification Error')\n",
    "    SVM_plot.plot(Ks, mse_SVM_test, label = 'SVM Test Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.xlabel('Parámetro Regularizador $C$')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.axis([1,6,0.0, 0.7])\n",
    "    plt.xticks(Ks, Ks_ticks)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.xlabel('Parametro Regularización $C$')\n",
    "    #plt.ylabel('Miss Classification Error')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    Depths = range(1,30)\n",
    "\n",
    "    mse_DTC_train = []\n",
    "    mse_DTC_test = []\n",
    "\n",
    "    for i in Depths:\n",
    "        model_DTC = DTC(max_depth = i)\n",
    "\n",
    "        model_DTC.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_DTC_train = model_DTC.predict(X_train)\n",
    "        y_pred_DTC_test = model_DTC.predict(X_test)\n",
    "\n",
    "        mse_DTC_train.append(1-accuracy_score(y_train, y_pred_DTC_train))\n",
    "        mse_DTC_test.append(1-accuracy_score(y_test, y_pred_DTC_test))\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    DTC_plot = plt.subplot(1,1,1)\n",
    "    DTC_plot.plot(Depths, mse_DTC_train, label='Training Misclassification Error')\n",
    "    DTC_plot.plot(Depths, mse_DTC_test, label='Testing Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.xlabel('Depth')\n",
    "    plt.axis([0,30,0.0, 0.7])\n",
    "\n",
    "    plt.show()\n",
    "LR_SVM_DTC(X_lda_train,y_train,X_lda_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   <ul>\n",
    "       <li>LR:\n",
    "           <ul>\n",
    "             <li>Error de Entrenamiento: Al empezar mejora de un 0.55 a un 0.05 aproximadamente y se mantiene constante, logrando una notable diferencia con respecto al error de 0.4 obtenido anteriormente en el punto k).</li>\n",
    "             <li>Error de Test: Al empezar mejora de un 0.6 a un 0.3 aproximadamente y se mantiene constante en 0.3, obteniendo un mejor resultado que el anterior, ya que este convergia a 0.4.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "       <li>SVM:\n",
    "            <ul>\n",
    "             <li>Error de Entrenamiento: empieza con el mismo error pero converge más rápido a 0 a medida de que el parámetro regularizador $C$ aumenta. Anteriormente se tenía que con un $C > 1000$ convergía a 0, en cambio ahora con los datos ajustados con LDA, basta con un $C = 30$.</li>\n",
    "             <li>Error de Test: al igual que en el error de entrenamiento, empieza con el mismo error pero logra un error menor a medida de que el parámetro regularizador $C$ crece. En este caso, el mínimo error de test es de un 0.4, una décima más grande que el error anterior.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "       <li>DTC:\n",
    "            <ul>\n",
    "             <li>Error de Entrenamiento: converge de manera más rápida, ya que con profundida igual a 5, el error de entrenamiento alcanza el valor 0.</li>\n",
    "             <li>Error de Test: En este caso, aumenta el error de test en comparación en el punto l) en aproximadamente 0.05 décimas.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "   </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(p)**  Intente mejorar el desempeño de los algoritmos ya entrenados. Diseñe ahora sus propias características (_feature crafting_) a partir de los datos brutos (secuencia de amplitudes), puede inspirarse en otros trabajos [6] [7] si desea.\n",
    "</p>\n",
    "\n",
    "- [6]  https://www.kaggle.com/primaryobjects/voicegender/data\n",
    "- [7]  Gamit, M. R., Dhameliya, P. K., & Bhatt, N. S. (2015). Classification Techniques for Speech Recognition: A Review. vol, 5, 58-63.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import skew\n",
    "from scipy import stats\n",
    "from statistics import mode, StatisticsError\n",
    "\n",
    "def centroide(X,Y):\n",
    "    return [sum(X)/len(X),sum(Y)/len(Y)]\n",
    "\n",
    "df = pd.read_csv('./heartbeat-sounds/set_a.csv')\n",
    "df.drop(['dataset','sublabel',],axis=1,inplace=True)\n",
    "new_df =pd.DataFrame({'file_name' : df['fname'].apply(clean_filename,string='Aunlabelledtest')})\n",
    "new_df['time_series'] = new_df['file_name'].apply(load_wav_file, path='./heartbeat-sounds/set_a/')\n",
    "new_df['len_series'] = new_df['time_series'].apply(len)\n",
    "new_df['time_series'] = new_df['time_series'].apply(padd_zeros,length=max(new_df['len_series']))\n",
    "new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "             1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
    "             2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
    "             1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0,\n",
    "             2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0,\n",
    "             0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "labels = ['artifact','normal/extrahls', 'murmur']\n",
    "new_df['target'] = [labels[i] for i in new_labels]\n",
    "new_df[\"target\"] = new_df[\"target\"].astype('category')\n",
    "cat_columns = new_df.select_dtypes(['category']).columns\n",
    "new_df[cat_columns] = new_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "new_df.head()\n",
    "new_df = new_df.sample(frac=1,random_state=44)\n",
    "X = np.stack(new_df['time_series'].values, axis=0)\n",
    "y = new_df.target.values\n",
    "data = np.zeros((X.shape[0],13))\n",
    "\n",
    "for i in range(0,X.shape[0]):\n",
    "    data[i][0] = np.mean(X[i])\n",
    "    data[i][1] = np.median(X[i])\n",
    "    data[i][2] = np.std(X[i])\n",
    "    data[i][3] = (pd.Series(X[i])).quantile(0.25)\n",
    "    data[i][4] = (pd.Series(X[i])).quantile(0.75)\n",
    "    data[i][5] = iqr(X[i])\n",
    "    data[i][6] = skew(X[i])\n",
    "    try:\n",
    "        data[i][7] = mode(X[i])\n",
    "    except StatisticsError:\n",
    "        data[i][7] = 0\n",
    "        \n",
    "    data[i][8] = min(X[i])    \n",
    "    data[i][9] = max(X[i])\n",
    "    data[i][10] = np.mean(np.fft.fftfreq(len(X[i])))\n",
    "    if i < X.shape[0]-1:\n",
    "        data[i][11] = centroide(X[i],X[i+1])[0]\n",
    "        data[i][12] = centroide(X[i],X[i+1])[1]\n",
    "        \n",
    "\n",
    "#https://www.kaggle.com/primaryobjects/voicegender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = [\"Mean\",\"Median\", \"Std\",\"Q25\",\"Q75\",\"IQR\",\"Skew\",\"Mode\",\"Min\",\"Max\",\"MFreq\",\"Centroide i\", \"Centroide i+1\"]\n",
    "df2 = pd.DataFrame(data = data, columns = headers)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_fourier = np.abs(np.fft.fft(data))\n",
    "data_resampled = []\n",
    "for i in range(X_fourier.shape[0]):\n",
    "    sequence = data_fourier[i,:].copy()\n",
    "    resampled_sequence = signal.resample(sequence, 100000)\n",
    "    data_resampled.append(resampled_sequence)\n",
    "\n",
    "data_resampled = np.array(data_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.25, random_state=43)\n",
    "print(\"El set de entrenamiento tiene \" + str(len(X_train)) \n",
    "      + \" y el set de test tiene \" + str(len(X_test)) \n",
    "      + \" datos, y ambos tienen 10000 clases\")\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=2)\n",
    "pca_model.fit(X_train,y_train)\n",
    "X_pca_train = pca_model.transform(X_train)\n",
    "X_pca_test = pca_model.transform(X_test)\n",
    "plot(X_pca_train,y_train)\n",
    "plot(X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_SVM_DTC(X_pca_train,y_train,X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   <ul>\n",
    "       <li>LR:\n",
    "           <ul>\n",
    "             <li>Error de Test: Se obtiene un menor error de test que con los datos anteriormente seleccionados, al ocupar un parámetro regularizador de 0.0001 se obtiene el menor error de 0.3.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "       <li>SVM:\n",
    "            <ul>\n",
    "             <li>Error de Test: No se mejora mucho en comparación con los datos anteriores, pero se obtiene un error de 0.4 aproximadamente cuando se ocupa un parámetro regularizador $10 <= C <= 100$.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "       <li>DTC:\n",
    "            <ul>\n",
    "             <li>Error de Test: Se obtiene el mínimo error de test, un 0.3</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "   </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = LDA(n_components=2)\n",
    "lda_model.fit(X_train,y_train)\n",
    "X_lda_train = lda_model.transform(X_train)\n",
    "X_lda_test = lda_model.transform(X_test)\n",
    "plot(X_lda_train,y_train)\n",
    "plot(X_lda_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_SVM_DTC(X_lda_train,y_train,X_lda_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Análisis de emociones en tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)**  Construya un dataframe con los datos a analizar. Determine cuántas clases existen, cuántos registros por clase y describa el _dataset_.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('./text_emotion.csv')\n",
    "df_copia = df.copy()\n",
    "valores = df.sentiment.value_counts()\n",
    "def plot_valores(valores):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    location = np.arange(len(valores))\n",
    "    width = 0.45\n",
    "    uno = ax.bar(location, valores.values(),  width, align='center', color=\"#F5A9F2\")\n",
    "    ax.set_xticks(location)\n",
    "    ax.set_xticklabels(list(valores.keys()), fontsize=15)\n",
    "    ax.set_title('Data Distribution')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plot_valores(dict(valores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Sentimiento | Registro |\n",
    "|:-----------:|:--------:|\n",
    "|   Neutral   |   8638   |\n",
    "|    Worry    |   8459   |\n",
    "|  Happiness  |   5209   |\n",
    "|   Sadness   |   5165   |\n",
    "|     Love    |   3842   |\n",
    "|   Surprise  |   2187   |\n",
    "|     Fun     |   1776   |\n",
    "|    Relief   |   1526   |\n",
    "|     Hate    |   1323   |\n",
    "|    Empty    |    827   |\n",
    "|  Enthusiasm |    759   |\n",
    "|   Boredom   |    179   |\n",
    "|    Anger    |    110   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)**  Construya un conjunto de entrenamiento y otro de pruebas, a través de una máscara aleatoria, para verificar los resultados de los algoritmos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(22)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=43)\n",
    "\n",
    "#print(df['content'][2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)**  Implemente y explique un pre-procesamiento para los tweets para dejarlos en un formato estándarizado\n",
    "en el cual se podrán trabajar.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['tweet_id','author'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\[\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "#print(preprocess(df['content'][0]))\n",
    "\n",
    "#df['content'] = df['content'].str\n",
    "df['content'] = df['content'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = [\"rt\",\"via\",\"a\",\"able\",\"about\",\"across\",\"after\",\"all\",\"almost\",\"also\",\"am\",\"among\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\"be\",\"because\",\"been\",\"but\",\"by\",\"can\",\"cannot\",\"could\",\"dear\",\"did\",\"do\",\"does\",\"either\",\"else\",\"ever\",\"every\",\"for\",\"from\",\"get\",\"got\",\"had\",\"has\",\"have\",\"he\",\"her\",\"hers\",\"him\",\"his\",\"how\",\"however\",\"i\",\"if\",\"in\",\"into\",\"is\",\"it\",\"its\",\"just\",\"least\",\"let\",\"like\",\"likely\",\"may\",\"me\",\"might\",\"most\",\"must\",\"my\",\"neither\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"often\",\"on\",\"only\",\"or\",\"other\",\"our\",\"own\",\"rather\",\"said\",\"say\",\"says\",\"she\",\"should\",\"since\",\"so\",\"some\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"there\",\"these\",\"they\",\"this\",\"tis\",\"to\",\"too\",\"twas\",\"us\",\"wants\",\"was\",\"we\",\"were\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"would\",\"yet\",\"you\",\"your\"]\n",
    "stop = stopwords.words('english')\n",
    "result = list(set(stop)|set(stop_words)|set(list('abcdefghijklmnopqrstuvwxyz')) | set(list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')) | set(list('0123456789')))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda x: [item for item in x if item not in result])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)**  Haga una reducción binaria al problema, para trabajarlo como un problema de clasificación de dos clases.\n",
    "Para esto, agrupe las distintas emociones como positivas y negativas (defina un criterio), se recomienda\n",
    "codificar las clases como +1 y −1 respectivamente. Recuerde tener presente que el desbalanceo de los\n",
    "datos puede afectar considerablemente al modelo.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uno = ['happiness', 'love', 'fun', 'relief', 'enthusiasm']\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x in uno else -1)\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)**  Para construir un clasificador que determine automáticamente la polaridad de un trozo de texto, será\n",
    "necesario representar los tweets $\\{t_i\\}_{i=1}^{n}$ disponibles como vectores de características (features). El tipo\n",
    "de características más utilizado consiste en contar cuántas veces aparecen ciertos términos/palabras en\n",
    "el texto. Para esto, es necesario un vocabulario que, por lo general, se construye mediante la unión de\n",
    "todas las palabras que se observen en los tweets.\n",
    "\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\">\n",
    "Se recomienda utilizar las librerías ofrecidas por sklearn de feature extraction in text [12] (_CountVectorizer_ y _TfidfVectorizer_). Recuerde realizar el ajuste (_fit_) únicamente con el conjunto de entrenamiento, para luego transformar el conjunto de pruebas (con el método transform).\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "- [12] http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "X = df['content'].values\n",
    "y = df.sentiment.values\n",
    "XX = df_copia['content'].values\n",
    "yy = y.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "XX_train, XX_test, yy_train, yy_test = train_test_split(XX, yy, test_size=0.3, random_state=22)\n",
    " \n",
    "def fake(x):\n",
    "    return x\n",
    "\n",
    "#vectorizer = TfidfVectorizer(tokenizer=fake, preprocessor=fake, lowercase=False)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=fake, preprocessor=fake, lowercase=False)\n",
    "#CV_fit = CV_model.fit_transform(X_train.tostring())\n",
    "#X_CV_train = CV_model.transform(X_train)\n",
    "\n",
    "X_CV_train = vectorizer.fit_transform(X_train)\n",
    "X_CV_test = vectorizer.transform(X_test)\n",
    "\n",
    "type(X_CV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_model.fit(X_train.toarray(), df_train['sentiment'])\n",
    "\n",
    "qda_score_train = qda_model.score(X_train.toarray(), df_train['sentiment'])\n",
    "qda_score_test = qda_model.score(X_test.toarray(), df_test['sentiment'])\n",
    "\n",
    "print(\"NB Train score %f\"%qda_score_train)\n",
    "print(\"NB Test score %f\"%qda_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train.toarray(), df_train['sentiment'])\n",
    "\n",
    "lda_score_train = lda_model.score(X_train, df_train['sentiment'])\n",
    "lda_score_test = lda_model.score(X_test, df_test['sentiment'])\n",
    "\n",
    "print(\"NB Train score %f\"%lda_score_train)\n",
    "print(\"NB Test score %f\"%lda_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)**  Entrene y compare al menos 5 de los diferentes clasificadores vistos en clases para clasificación binaria (por ejemplo: Navie Bayes, Multinomial Naive Bayes, LDA, QDA, Regresión logística, SVM y Arboles de decisión) sobre el conjunto de entrenamiento verificando su desempeño sobre ambos conjuntos (entrenamiento y de pruebas), construyendo un gráfico resumen del error de éstos.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_comparation(ScoreTrain, ScoreTest):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "    location = np.arange(len(ScoreTrain))\n",
    "    width = 0.45\n",
    "    \n",
    "    uno = ax.barh(location, ScoreTrain.values(),  width, align='center', color=\"#F5A9F2\")\n",
    "    dos = ax.barh(location + width, ScoreTest.values(), width, align='center', color=\"#A9F5F2\")\n",
    "    ax.set_yticks(location + width/2)\n",
    "    ax.set_yticklabels((\"NB\", \"DTC\", \"LR\", \"KNC\", \"MLPC\"))\n",
    "    \n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title('Train and Test Score')\n",
    "    ax.legend((uno[0], dos[0]), ('Train', 'Test'), bbox_to_anchor=(1,0.6))\n",
    "    #autolabel(uno)\n",
    "    #autolabel(dos)\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "ScoreTrain = {}\n",
    "ScoreTest = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_model = BernoulliNB()\n",
    "nb_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"NB Train\"] = nb_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"NB Test\"] = nb_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"NB Train score %f\"%ScoreTrain[\"NB Train\"])\n",
    "print(\"NB Test score %f\"%ScoreTest[\"NB Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "DTC_model = DTC()\n",
    "DTC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"DTC Train\"] = DTC_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"DTC Test\"] = DTC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"DTC Train score %f\"%ScoreTrain[\"DTC Train\"])\n",
    "print(\"DTC Test score %f\"%ScoreTest[\"DTC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "LR_model = LR()\n",
    "LR_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"LR Train\"] = LR_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"LR Test\"] = LR_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"LR Train score %f\"%ScoreTrain[\"LR Train\"])\n",
    "print(\"LR Test score %f\"%ScoreTest[\"LR Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "KNC_model = KNC()\n",
    "KNC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"KNC Train\"] = KNC_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"KNC Test\"] = KNC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"KNC Train score %f\"%ScoreTrain[\"KNC Train\"])\n",
    "print(\"KNC Test score %f\"%ScoreTest[\"KNC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "MLPC_model = MLPC()\n",
    "MLPC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"MLPC Train\"] = MLPC_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"MLPC Test\"] = MLPC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"MLPC Train score %f\"%ScoreTrain[\"MLPC Train\"])-\n",
    "print(\"MLPC Test score %f\"%ScoreTest[\"MLPC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_comparation(ScoreTrain, ScoreTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(g)**  Utilice y explique las métricas que calcula la función classification report de la librería sklearn. En base\n",
    "a las distintas métricas calculadas ¿Cuáles clasificadores son los que mejor se comportan?\n",
    "</p>\n",
    "\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    _Classification Report_ entrega las metricas principales de los clasificadores, es decir, un resumen de la precisión, el recall y el valor F.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    En este contexto se denomina precisión (denominado igualmente valor positivo predicho) como a la fracción de instancias recuperadas que son relevantes, mientras recall (denominado igualmente sensibilidad o exhaustividad) es la fracción de instancias relevantes que han sido recuperadas.\n",
    "</p>\n",
    "\n",
    "<img src=\"precisionrecall.png\">\n",
    "\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    El valor F (en ingles F-score) es la medida de precisión que tiene un test. Considera el valor de precisión $p$ y el de recall $r$ de test para calcular el valor.\n",
    "</p>\n",
    "\n",
    "<img src=\"F1.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def score_the_model(model,x,y,xt,yt):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print(\"___________________________________________________________________________\")\n",
    "    print(\"Training Accuracy: %f\"%(acc_tr))\n",
    "    print(\"Test Accuracy: %f\"%(acc_test))\n",
    "    print(\"Detailed Analysis Testing Results ...\")\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))\n",
    "    \n",
    "score_the_model(nb_model, X_CV_train, y_train, X_CV_test, y_test)\n",
    "score_the_model(DTC_model, X_CV_train, y_train, X_CV_test, y_test)\n",
    "score_the_model(LR_model, X_CV_train, y_train, X_CV_test, y_test)\n",
    "score_the_model(KNC_model, X_CV_train, y_train, X_CV_test, y_test)\n",
    "score_the_model(MLPC_model, X_CV_train, y_train, X_CV_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(h)**  [Opcional] Visualice las predicciones de algún modelo generativo (probabilístico) definido anteriormente, tomando un subconjunto aleatorio de tweets de pruebas y explorando las probabilidades que asigna el clasificador a cada clase.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "test_pred = LR_model.predict_proba(X_CV_test)\n",
    "#print(test_pred)\n",
    "random.seed(22)\n",
    "spl = random.sample(range(len(test_pred)), 15)\n",
    "\n",
    "\n",
    "print(\"Negative \\t Positive \\t Tweet\")\n",
    "for i in spl:\n",
    "    print(test_pred[i],\"\\t\",XX_test[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(i)**  Ahora deberá extender el problema a las múltiples clases que tiene presente (las distintas emociones),\n",
    "es decir, su trabajo será el de predecir una de las distintas emociones de cada _tweet_. Para esto utilice el\n",
    "mismo pre-procesamiento realizado en el punto c) y las características generadas mediante las técnicas\n",
    "en el punto e). Recuerde que tendrá que codificar las distintas clases como valores numéricos enteros.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('./text_emotion.csv')\n",
    "df.shape\n",
    "np.random.seed(22)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df.drop(['tweet_id','author'],axis=1,inplace=True)\n",
    "df['content'] = df['content'].apply(preprocess)\n",
    "df['content'] = df['content'].apply(lambda x: [item for item in x if item not in result])\n",
    "\n",
    "#Sentimientos\n",
    "#Neutral, Worry, Happiness, Sadness, Love, Surprise, Fun, Relief, Hate, Empty, Enthusiasm, Boredom, Anger\n",
    "\n",
    "#Propuesta orden de sentimiento\n",
    "#Happiness, Fun, Love, Surprise, Enthusiasm, Relief, Neutral, Empty,  Boredom, Worry, Sadness, Hate, Anger\n",
    "#\"Happiness\", \"Fun\", \"Love\", \"Surprise\", \"Enthusiasm\", \"Relief\", \"Neutral\", \"Empty\", \"Boredom\", \"Worry\", \"Sadness\", \"Hate\", \"Anger\"\n",
    "\n",
    "emotions = [\"Happiness\", \"Love\", \"Fun\", \"Surprise\", \"Enthusiasm\", \"Relief\", \"Neutral\", \"Empty\", \"Boredom\", \"Worry\", \"Sadness\", \"Anger\", \"Hate\"]\n",
    "\n",
    "emotions =  [x.lower() for x in reversed(emotions)]\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: emotions.index(x)-6)\n",
    "df.head()\n",
    "\n",
    "X = df['content'].values\n",
    "y = df.sentiment.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "\n",
    "def fake(x):\n",
    "    return x\n",
    "\n",
    "#vectorizer = TfidfVectorizer(tokenizer=fake, preprocessor=fake, lowercase=False)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=fake, preprocessor=fake, lowercase=False)\n",
    "\n",
    "X_CV_train = vectorizer.fit_transform(X_train)\n",
    "X_CV_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(j)**  Utilice los clasificadores que son extendidos por defecto a múltiples clases para detectar emociones en\n",
    "cada _tweet_, muestre sus desempeños a través del error de pruebas en un gráfico resumen.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ScoreTrain_M = {}\n",
    "ScoreTest_M = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "DTC_model = DTC()\n",
    "DTC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"DTC Train\"] = DTC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"DTC Test\"] = DTC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"DTC Train score %f\"%ScoreTrain_M[\"DTC Train\"])\n",
    "print(\"DTC Test score %f\"%ScoreTest_M[\"DTC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "KNC_model = KNC()\n",
    "KNC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"KNC Train\"] = KNC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"KNC Test\"] = KNC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"KNC Train score %f\"%ScoreTrain_M[\"KNC Train\"])\n",
    "print(\"KNC Test score %f\"%ScoreTest_M[\"KNC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "MLPC_model = MLPC()\n",
    "MLPC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"MLPC Train\"] = MLPC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"MLPC Test\"] = MLPC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"MLPC Train score %f\"%ScoreTrain_M[\"MLPC Train\"])\n",
    "print(\"MLPC Test score %f\"%ScoreTest_M[\"MLPC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "MNB_model = MNB()\n",
    "MNB_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"MNB Train\"] = MNB_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"MNB Test\"] = MNB_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"MNB Train score %f\"%ScoreTrain_M[\"MNB Train\"])\n",
    "print(\"MNB Test score %f\"%ScoreTest_M[\"MNB Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(k)**  Utilice clasificadores binarios que pueden ser extendidos a través de otras técnicas, tal como One vs\n",
    "One y One vs All/Rest [14]\n",
    "</p>\n",
    "\n",
    "- [14]  http://scikit-learn.org/stable/modules/classes.html#module-sklearn.multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier as ORC\n",
    "ORC_model = ORC(MNB(), -1)\n",
    "ORC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"ORC Train\"] = ORC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"ORC Test\"] = ORC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"ORC Train score %f\"%ScoreTrain_M[\"ORC Train\"])\n",
    "print(\"ORC Test score %f\"%ScoreTest_M[\"ORC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier as OOC\n",
    "OOC_model = OOC(MNB(), -1)\n",
    "OOC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"OOC Train\"] = OOC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"OOC Test\"] = OOC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"OOC Train score %f\"%ScoreTrain_M[\"OOC Train\"])\n",
    "print(\"OOC Test score %f\"%ScoreTest_M[\"OOC Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(l)**  Para el caso de la Regresión Logística compare sus dos métodos para ser extendidos a múltiples clases.\n",
    "Uno a través de One vs Rest y otro definiendo que la variable a predecir se distribuye Multinomial.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "LR_model = LR(multi_class='ovr')\n",
    "LR_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"LR OvR Train\"] = LR_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"LR OvR Test\"] = LR_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"LR Train score %f\"%ScoreTrain_M[\"LR OvR Train\"])\n",
    "print(\"LR Test score %f\"%ScoreTest_M[\"LR OvR Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_model = LR(multi_class='multinomial', solver=\"newton-cg\")\n",
    "#Currently the ‘multinomial’ option is supported only by the ‘lbfgs’ and ‘newton-cg’ solvers.\n",
    "LR_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"LR Multi Train\"] = LR_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"LR Multi Test\"] = LR_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"LR Train score %f\"%ScoreTrain_M[\"LR Multi Train\"])\n",
    "print(\"LR Test score %f\"%ScoreTest_M[\"LR Multi Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(m)**  Compare los resultados entre los clasificadores extendidos por defecto y los binarios que son extendidos mediante otras técnicas, construya una tabla o gráfico resumen. Los clasificadores que mejor se comportan en el caso binario ¿Siguen teniendo ese desempeño en múltiples clases?\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_comparation_M(ScoreTrain, ScoreTest):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "    location = np.arange(len(ScoreTrain))\n",
    "    width = 0.45\n",
    "    \n",
    "    uno = ax.barh(location, ScoreTrain.values(),  width, align='center', color=\"#F5A9F2\")\n",
    "    dos = ax.barh(location + width, ScoreTest.values(), width, align='center', color=\"#A9F5F2\")\n",
    "    ax.set_yticks(location + width/2)\n",
    "    ax.set_yticklabels((\"DTC\", \"KNC\", \"MLPC\", \"MNB\", \"ORC\", \"OOC\", \"LR OvR\", \"LR Multi\"))\n",
    "    \n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title('Train and Test Score')\n",
    "    ax.legend((uno[0], dos[0]), ('Train', 'Test'), bbox_to_anchor=(1,0.6))\n",
    "    #autolabel(uno)\n",
    "    #autolabel(dos)\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plot_comparation_M(ScoreTrain_M, ScoreTest_M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
