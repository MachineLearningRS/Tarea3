{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div><img src=\"escudo_utfsm.gif\" style=\"float:right;height:100px\">\n",
    "<img src=\"IsotipoDIisocolor.png\" style=\"float:left;height:100px\">\n",
    "<center>\n",
    "    <h1> INF-493 - Machine Learning</h1>\n",
    "    <h1> Tarea 2 - Métodos para Clasificación </h1>\n",
    "\n",
    "<p>\n",
    "<br><center>_Javier Reyes_<strong> - </strong>_javier.reyes.12@sansano.usm.cl_<strong> - </strong>_201273524-6_ </center>\n",
    "<br><center>_Marco Salinas_<strong> - </strong>_marco.salinas.12@sansano.usm.cl_<strong> - </strong>_201273589-0_ </center>\n",
    "</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1 Tipos de Fronteras en Clasificación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "n_samples=500\n",
    "mean = (0,-4)\n",
    "C = np.array([[0.3, 0.1], [0.1, 1.5]])\n",
    "datos1 = np.random.multivariate_normal(mean, C, n_samples)\n",
    "outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples))*3\n",
    "outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples))*3\n",
    "datos2 = np.vstack((outer_circ_x,outer_circ_y)).T\n",
    "generator = check_random_state(10)\n",
    "datos2 += generator.normal(scale=0.3, size=datos2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)** Construya el conjunto de datos (dataset) común con los dos conjuntos generados. Luego se realiza un\n",
    "shift desde el conjunto 2 al 1, esto se puede ver en la imagen anterior, donde el conjunto de color\n",
    "naranjo (media luna) tiene puntos azules a la derecha pertenecientes al otro conjunto, esto es con\n",
    "el mismo propósito de trabajar con un dataset no ideal. Determine cuántos registros contiene cada\n",
    "conjunto y visualícelos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.concatenate((datos1, datos2), axis=0)\n",
    "n = 20 #ruido/noise\n",
    "y1 = np.zeros(datos1.shape[0]+n)\n",
    "y2 = np.ones(datos2.shape[0]-n) #media luna\n",
    "y = np.concatenate((y1,y2),axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "\n",
    "def plot(x,y):\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    plt.scatter(x[:,0], x[:,1], s=50, c=y, cmap=plt.cm.Set3)\n",
    "    plt.title(\"Distribución del dataset\")\n",
    "    plt.show()\n",
    "    \n",
    "plot(X,y)\n",
    "    \n",
    "print(\"El primer conjunto tiene \" + str(len(y1))\n",
    "     + \" datos \\nEl segundo conjunto tiene \" + str(len(y2)) + \" datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)** Entrene el clasificador Linear Discriminant Analysis (LDA) y visualice la frontera de decisión que\n",
    "define este algoritmo. Analice cualitativamente lo que observa.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.mlab as mlab\n",
    "def visualize_border(model,x,y,title=\"\"):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.scatter(x[:,0], x[:,1], s=50, c=y, cmap=plt.cm.Set3)\n",
    "    h = .02 # step size in the mesh\n",
    "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.cool)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "model_LDA = LDA()\n",
    "model_LDA.fit(X_train,y_train)\n",
    "visualize_border(model_LDA,X_train,y_train,\"LDA\")\n",
    "visualize_border(model_LDA,X_test,y_test,\"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    Se puede observar que el conjuto de datos 1 esta separado de una manera uniforme, pero en uno de sus extremos se mezclan con los datos del conjunto 2, haciendo imposible una separación para discriminar los datos con el algoritmo de LDA (a través de una recta). La única manera de poder utilizar el clasificador, es eliminando los outlayers, aunque esto es muy riesgoso porque pueden ser información valiosa.\n",
    "</p>\n",
    "    \n",
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)** Entrene el clasificador Quadratic Discriminant Analysis (QDA) y visualice la frontera de decisión que define este algoritmo. Analice cualitativamente lo que observa y compare con LDA, en qué difieren y en qué se asemejan ¿Qué distribución de probabilidad asumen cada uno?\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "import math as ma\n",
    "model_QDA = QDA()\n",
    "model_QDA.fit(X_train,y_train)\n",
    "visualize_border(model_QDA,X_train,y_train,\"QDA\")\n",
    "visualize_border(model_QDA,X_test,y_test,\"QDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    Al igual que en el caso de LDA, la separación entre ambos conjuntos es imposible, pero en este caso, QDA logra delimitar de mejor manera los conjutos integrando más datos al conjunto de datos azules. \n",
    "</p>\n",
    "\n",
    "<p  style=\"text-align: justify;\">\n",
    "    <ul>\n",
    "        <li>Semejanzas\n",
    "            <ul>\n",
    "                <li>Ambos métodos de clasificación no pueden separar en un 100% el conjunto de datos.</li>\n",
    "                <li>Aunque el tipo de separación es distinta, la cantidad de puntos que separan es prácticamente la misma.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Diferencias\n",
    "            <ul>\n",
    "                <li>El tipo de separación para clasificar los datos es distinta: LDA - línea recta; QDA - parábola.</li>\n",
    "                <li>QDA prioriza la separación en donde está más centrado un tipo de datos, mientras que LDA tiende a separar a la \"mitad\" el dataset.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\">\n",
    "    Tanto LDA como QDA asumen una distribución de probabilidad normal o Gaussiana.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)** Compare cuantitativamente los clasificadores LDA y QDA en este dataset sintético mediante la métrica\n",
    "de error de clasificación.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_qda_train= model_QDA.predict(X_train)\n",
    "y_pred_lda_train = model_LDA.predict(X_train)\n",
    "y_pred_qda_test = model_QDA.predict(X_test)\n",
    "y_pred_lda_test = model_LDA.predict(X_test)\n",
    "print(\"Miss Classification Loss for LDA Train: %f\"%(1-accuracy_score(y_train, y_pred_lda_train)))\n",
    "print(\"Miss Classification Loss for QDA Train: %f\"%(1-accuracy_score(y_train, y_pred_qda_train)))\n",
    "print(\"Miss Classification Loss for LDA Test: %f\"%(1-accuracy_score(y_test, y_pred_lda_test)))\n",
    "print(\"Miss Classification Loss for QDA Test: %f\"%(1-accuracy_score(y_test, y_pred_qda_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)** Construya una función que entrene/ajuste un modelo de Regresión Logística Regularizado (utilizando\n",
    "como penalizador la norma $l_2$), experimente con distintos valores del parámetro de regularización\n",
    "mediante el gráfico interactivo. Explique el significado y efecto esperado de este parámetro. Analice\n",
    "cualitativamente lo observado.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "def visualize_border_interactive(param):\n",
    "    model = train_model(param)\n",
    "    visualize_border(model,X_train,y_train)\n",
    "    visualize_border(model,X_test,y_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "def train_model(param):\n",
    "    model=LR(C=param,penalty='l2')\n",
    "    model.fit(X_train,y_train)\n",
    "    return model\n",
    "\n",
    "p_min = 0.01\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive,param=(p_min,p_max, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "Al aplicar la penalización con la norma $l_2$, lo que hace la $Logistic Regression$ es ir penalizando los datos con la regla de $Lasso$, es decir, que a mayor $\\lambda$ mayor es la penalización. Es por esto que al aumentar la penalización, los límites de separación se irán ajustando de manera que quede lo más equilibrado posible, aunque esto implique \"pasarse\" un poco de la pendiente normal que tenía la recta al utilizar LDA.\n",
    "<br>\n",
    "Como se puede observar, en $p_{min}$ se obtiene la separación con LDA y mientras aumenta el valor de la penalización hasta llegar al valor $p_{max}$ se tendrá la \"mejor\" separación o la más equitativa entre los datos. Notar que llegando al valor $p = 0.6$ luego si se sigue aumentando el valor de $p$, la pendiente de la recta no aumentará, por lo que se llegá a la mejor separación entre los límites de los conjuntos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)** Construya una función que entrene/ajuste una Máquina de Vectores de Soporte (SVM) Lineal. Mediante\n",
    "la imagen interactiva explore diferentes valores del parámetro de regularización C. Discuta el significado\n",
    "y efecto esperado de este parámetro. Analice cualitativamente lo observado.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "def train_model(param):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel='linear')\n",
    "    model.fit(X_train,y_train)\n",
    "    return model\n",
    "\n",
    "#use interactive\n",
    "p_min = 0.0001\n",
    "p_max = 1\n",
    "interactive(visualize_border_interactive,param=(p_min,p_max, 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "Las $SMV$ se caracterizan por maximizar la mínima distancia de un punto de un conjuto al hiperplano, por lo que en este caso, se tendrá que aplicar el parámetro $C$ para poder clasificar de una mejor manera los datos. La función que cumple el parámetro $C$ es similar a la norma $l_2$ en $Logistic Regression$, ya que este se encarga de decirle a la $SVM$ cuántos datos mal clasificados quieres evitar. Si se tiene un $C$ pequeño, se tendrá un mayor margen de la separación al hiperplano incluso si hay elementos mal clasificados, en cambio con un $C$ grande, la $SVM$ buscará el margen al hiperplano más pequeño.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(g)** Construya una función que entrene/ajuste una Máquina de Vectores de Soporte (SVM) no Lineal.\n",
    "Mediante la imagen interactiva explore diferentes valores del parámetro de regularización C y con\n",
    "diferentes kernels. Discuta el significado y efecto esperado de este parámetro. Analice cualitativamente\n",
    "lo observado.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_border_interactive(param, kernel):\n",
    "    model = train_model(param, kernel)\n",
    "    visualize_border(model,X_train,y_train)\n",
    "    visualize_border(model,X_test,y_test)\n",
    "\n",
    "def train_model(param, kernel):\n",
    "    model= SVM()\n",
    "    model.set_params(C=param,kernel=kernel)\n",
    "    model.fit(X_train,y_train)\n",
    "    return model\n",
    "\n",
    "#use interactive\n",
    "p_min = 1\n",
    "p_max = 100\n",
    "interactive(visualize_border_interactive,param=(p_min,p_max, 1), kernel = ['rbf','poly','linear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n",
    "\n",
    "<p  style=\"text-align: justify;\">\n",
    "    <ul>\n",
    "        <li>C - RBF: Para el kernel <i>Radial Basic Function</i>, lo que hace $C$ es disminuir el \"radio\" que encierra al conjunto de datos amarillo. $C$ actúa de la misma manera anteriormente descrita, por lo que a un mayor $C$, el \"radio\" será menor.\n",
    "        </li>\n",
    "        <li>C - Poly: Para el kernel <i>Polinomial</i>, lo que hace $C$ es ir ajustando el polinomio a la separación óptima de los conjuntos de datos\n",
    "        </li>\n",
    "        <li>C - Linear: Es una $SVM$ normal, se discutió en la pregunta anterior.\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(h)** Construya un Arbol de Decisión de múltiples niveles para la clasificación del problema. Puede utilizar el criterio y la función de partición que prefiera. Mediante la imagen interactiva explore diferentes\n",
    "valores del parámetro de máxima profunidad del árbol. Discuta el significado y efecto esperado de este\n",
    "parámetro. Analice cualitativamente lo observado.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def visualize_border_interactive(param,criterion):\n",
    "    model = train_model(param,criterion)\n",
    "    visualize_border(model,X_train,y_train)\n",
    "    visualize_border(model,X_test,y_test)\n",
    "\n",
    "def train_model(param,criterion):\n",
    "    model = Tree() #edit the train_model function\n",
    "    model.set_params(max_depth=param,criterion='entropy',splitter='best')\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print (\"Train Accuracy is : {0:.1f}%\".format(accuracy_score(y_train,y_pred_train)*100))\n",
    "    print (\"Test Accuracy is : {0:.1f}%\".format(accuracy_score(y_test,y_pred_test)*100))\n",
    "    return model\n",
    "\n",
    "#use interactive\n",
    "p_min = 1\n",
    "p_max = 20\n",
    "interactive(visualize_border_interactive,param=(p_min,p_max, 1),criterion = ['gini','entropy','variance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    En este caso, los 3 criterios $gini$,$entropy$ y $variance$ se comportan de la misma manera en el Árbol de decisión. Lo que permite una mayor profundización en el árbol, es tener tantas separaciones de tal manera de obtener un conjunto de datos dentro de un espacio $A$ y otro en el espacio $B$, es decir, encacillar los conjuntos de datos.\n",
    "    <br>\n",
    "    Para este caso, el valor máximo de profundidad es $h_{max} = 11$ en donde se obtiene un 100% de acierto al separar el conjunto de datos amarillos del azul. Al seguir aumentando la cantidad de profundidad, lo único que se logra es separar los datos de otra manera.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(i)** Construya un algoritmo $k-NN$ para la clasificación del problema. Mediante la imagen interactiva explore\n",
    "diferentes valores del parámetro $k$. Discuta el significado y efecto esperado de este parámetro. Analice\n",
    "cualitativamente lo observado.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def visualize_border_interactive(param):\n",
    "    model = train_model(param)\n",
    "    visualize_border(model,X_train,y_train)\n",
    "    visualize_border(model,X_test,y_test)\n",
    "\n",
    "def train_model(param):\n",
    "    model = KNeighborsClassifier()\n",
    "    model.set_params(n_neighbors=param)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print (\"Train Accuracy is : {0:.1f}%\".format(accuracy_score(y_train,y_pred_train)*100))\n",
    "    print (\"Test Accuracy is : {0:.1f}%\".format(accuracy_score(y_test,y_pred_test)*100))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "k_min = 1\n",
    "k_max = 20\n",
    "interactive(visualize_border_interactive,param=(k_min,k_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "La fase de entrenamiento del algoritmo consiste en almacenar los vectores característicos y las etiquetas de las clases de los ejemplos de entrenamiento. En la fase de clasificación, la evaluación del ejemplo (del que no se conoce su clase) es representada por un vector en el espacio característico. Se calcula la distancia entre los vectores almacenados y el nuevo vector, y se seleccionan los $k$ ejemplos más cercanos. El nuevo ejemplo es clasificado con la clase que más se repite en los vectores seleccionados, es por esto que al aumentar la cantidad de vecinos $k$, el ruido o sesgo de los datos debería disminuir, aunque depende netamente de los datos.\n",
    "<br>\n",
    "EL algoritmo $k-NN$ actúa localmente buscando los vecinos más cercanos, los cuales se espera nos den la mejor clasificación. Para este caso, basta tomar $k = 1$ para obtener una separación perfecta entre los conjuntos de datos, ya que a medida de que se aumenta el número de vecino $k$, el porcentaje de acierto de clasificación llega como máximo al $98.5\\%$. Notar que cuando los vecinos son impares, el porcentaje de acierto es más alto. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Análisis de audios como datos brutos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    En esta parte de la experencia se trabajará con datos de audios los cuales son directamente extraídos desde datos fuentes _.wav_, lo que corresponde a una señal de sonido en diferentes tiempos. El _dataset_ se denomina _ Heartbeat Sounds_, el cual consta de grabaciones de sonidos de latidos cardíacos normales y anormales, con distintas categorías para los latidos anormales. Los datos fueron obtenidos desde un publico general a través de la aplicación de iPhone iStethoscope Pro.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "def clean_filename(fname, string):\n",
    "    file_name = fname.split('/')[1]\n",
    "    if file_name[:2] == '__':\n",
    "        file_name = string + file_name\n",
    "    return file_name\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "def load_wav_file(name, path):\n",
    "    s, b = wavfile.read(path + name)\n",
    "    assert s == SAMPLE_RATE\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)** Construya un dataframe con los datos a analizar. Describa el dataset y determine cuántos registros hay\n",
    "por clase.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('./heartbeat-sounds/set_a.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    El _dataset_ posee 176 datos, los cuales poseen 4 columnas cada uno.\n",
    "    <ul style=\"list-style-type:disc;margin-left:24px\">\n",
    "        <li>**dataset:** columna que indica a que dataset corresponde el dato</li>\n",
    "        <li>**fname:** corresponde al nombre del archivo de audio</li>\n",
    "        <li>**label:** puede ser \"normal\", estar en blanco (para datos sin etiqueta), y anormales donde pueden ser \"artifac\", \"extrahls\" y \"murmur\". </li>\n",
    "        <li>**sublabel:** columna sin datos.</li>\n",
    "\n",
    "    </ul>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p  style=\"text-align: justify;\">\n",
    "    Descripcion de las categorias:\n",
    "    <ul style=\"list-style-type:disc;margin-left:24px;text-align: justify;\">\n",
    "        <li>**Normal:** Corresponden a sonidos de corazones normales y saludables.</li>\n",
    "        <li>**Murmur (Soplo):** Corresponden a soplos cardíacos, que suenan como si hubiera un ruido de \"silbido, rugido, estruendo o fluido turbulento\". </li>\n",
    "        <li>**Extra Heart Sound:** Correspoden a una medición en la cual se escucha un sonido adicional del corazón.  </li>\n",
    "        <li>**Artifact:** En esta categoria se encuentra una amplia gama de sonidos diferentes, que incluyen chillidos y ecos de realimentación, voz, música y ruido. Gerenalmente no hay sonidos cardíacos discernibles. Esta categoría es la más diferente de las otras. </li>\n",
    "        <li>**Unlabelled:** Son aquellas muestras que no estan etiquetadas en ninguna categoria. </li>\n",
    "    </ul>\n",
    "</p>\n",
    "\n",
    "|     Categoria     \t| Registro \t|\n",
    "|:-----------------:\t|:--------:\t|\n",
    "|       Normal      \t|    31    \t|\n",
    "|       Murmur      \t|    34    \t|\n",
    "| Extra Heart Sound \t|    19    \t|\n",
    "|      Artifact     \t|    40    \t|\n",
    "|     Unlabelled    \t|    52    \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['dataset','sublabel',],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Artifact\",df.label.value_counts()['artifact'])\n",
    "print(\"Murmur\", df.label.value_counts()['murmur'])\n",
    "print(\"Extra Heart Sound\", df.label.value_counts()['extrahls'])\n",
    "print(\"Normal\", df.label.value_counts()['normal'])\n",
    "print(\"Unlabelled\", 176-40-34-19-31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)** Lea los archivos _.wav_ y transformelos en secuencias de tiempo. Realice un _padding_ de ceros al final de\n",
    "cada secuencia para que todas queden representadas con la misma cantidad de elementos, explique la\n",
    "importancia de realizar este paso.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padd_zeros(array,length):\n",
    "    aux = np.zeros(length)\n",
    "    aux[:array.shape[0]] = array\n",
    "    return aux\n",
    "\n",
    "\n",
    "new_df =pd.DataFrame({'file_name' : df['fname'].apply(clean_filename,string='Aunlabelledtest')})\n",
    "new_df['time_series'] = new_df['file_name'].apply(load_wav_file, path='./heartbeat-sounds/set_a/')\n",
    "new_df['len_series'] = new_df['time_series'].apply(len)\n",
    "new_df['time_series'] = new_df['time_series'].apply(padd_zeros,length=max(new_df['len_series']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    La importancia de realizar este paso, es normalizar la dimensionalidad de cada vector de datos, para así trabajar con todos en la misma dimensión.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)** Manipule los datos y cambie las etiquetas de los audios por otras asignadas por un doctor experto,\n",
    "el cual afirma que estos cambios son requeridos. Vuelva a determinar cuántos registros hay por clase.\n",
    "Nótese que ahora son 3 clases ¿Explique la problemática de tener etiquetas mal asignadas en los datos?\n",
    "¿Un solo dato puede afectar esto?\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "             1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
    "             2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
    "             1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0,\n",
    "             2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0,\n",
    "             0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "\n",
    "labels = ['artifact','normal/extrahls', 'murmur']\n",
    "new_df['target'] = [labels[i] for i in new_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Artifact\",new_df.target.value_counts()['artifact'])\n",
    "print(\"Murmur\", new_df.target.value_counts()['murmur'])\n",
    "print(\"Normal and EHSound\", new_df.target.value_counts()['normal/extrahls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          Categoria         \t| Registro \t|\n",
    "|:--------------------------:\t|:--------:\t|\n",
    "| Normal / Extra Heart Sound \t|    65    \t|\n",
    "|           Murmur           \t|    53    \t|\n",
    "|          Artifact          \t|    58    \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   Como ahora las etiquetas son 3, se particionaron la clasificación de datos anteriormente reasignandolos a las nuevas etiquetas. Esto puede provocar un problema en cuanto a la nueva clasificación, ya que las nuevas etiquetas pueden que no representen en su totalidad al dato asignado y quede mal asignado por haber particionado el set de clasificación anterior.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "   Un solo dato puede afectar el porcentaje de acierto que anteriormente se obtuvo con los 2 tipos de clasificaciones, porque como se explicó anteriormente, las nuevas clases podrían no representar en su totalidad al dato al momento de clasificarlo, aunque también, puede que se clasifique de mejor manera el set de datos. Un claro ejemplo es ver la asignación de los datos en una nueva Split de un árbol de decisión.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)**  Codifique las distintas clases a valores numéricos para que puedan ser trabajados por los algoritmos\n",
    "clasificadores.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"target\"] = new_df[\"target\"].astype('category')\n",
    "cat_columns = new_df.select_dtypes(['category']).columns\n",
    "new_df[cat_columns] = new_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)** Desordene los datos, evitando así el orden en el que vienen la gran mayoría de las etiquetas. Cree la\n",
    "matriz que conforma a los datos en sus dimensiones sin preprocesar, es decir, cada ejemplo es una\n",
    "secuencia de amplitudes en el tiempo. ¿Las dimensiones de ésta indica que puede generar problemas?\n",
    "¿De qué tipo?\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.sample(frac=1,random_state=44)\n",
    "X = np.stack(new_df['time_series'].values, axis=0)\n",
    "y = new_df.target.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   Puede generar problemas de computo y tiempo. Al agregar dimensionalidad a un problema, trae como beneficio un mayor porcentaje de acierto, porque hay una mayor cantidad de atributos con los cuales clasificar de manera correcta los datos, pero la desventaja es que mientras de más dimensión sea el conjunto de datos, mayor tiempo tomará en procesar la data y clasificarla, por lo que en general, no es recomendable aumentar la dimensionalidad.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)** Para pre procesar la secuencia en el tiempo realice una Transformada de Fourier discreta para pasar\n",
    "los datos desde el dominio de tiempos al dominio de frecuencias presentes en la señal de sonido.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fourier = np.abs(np.fft.fft(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(g)**  Para seguir con el pre procesamiento realice un muestreo representativo de los datos a través de una\n",
    "técnica de muestreo especializada en secuencias ¿En qué beneficia este paso? ¿Cómo podría determinar\n",
    "si el muestro es representativo?\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "X_resampled = []\n",
    "for i in range(X_fourier.shape[0]):\n",
    "    sequence = X_fourier[i,:].copy()\n",
    "    resampled_sequence = signal.resample(sequence, 100000)\n",
    "    X_resampled.append(resampled_sequence)\n",
    "\n",
    "X_resampled = np.array(X_resampled)\n",
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   Este paso benificia en el tiempo computacional que tomará clasificar los datos, ya que reduce la dimensionalidad a su cuarta parte arpoximadamente.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "   Habría que ver todos los atributos y ver qué representan cada uno de ellos, para ver si en el proceso de mezclarlos y obtener nuevos atributos y conseguir una menor dimensionalidad, no se perdieron atributos representativos dentro del conjunto de atributos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(h)**  Genere un conjunto de pruebas mediante la técnica _hold-out validation_ para verificar la calidad de los\n",
    "clasificadores. ¿Cuántas clases tiene y de qué tamaño queda cada conjunto?\n",
    "\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=43)\n",
    "print(\"El set de entrenamiento tiene \" + str(len(X_train)) \n",
    "      + \" y el set de test tiene \" + str(len(X_test)) \n",
    "      + \" datos, y ambos tienen 10000 clases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(i)**  Realice un proceso de estandarizar los datos para ser trabajados adecuadamente. Recuerde que solo se\n",
    "debe ajustar (calcular media y desviación estándar) con el conjunto de entrenamiento.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(j)**  Realice una reducción de dimensionalidad a través de la técnica PCA, para representar los datos en $d = 2$ dimensiones. Recuerde que solo se debe ajustar (encontrar las componentes principales) con el conjunto de entrenamiento. Visualice apropiadamente la proyección en 2 dimensiones.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "d=2\n",
    "pca_model = PCA(n_components=d)\n",
    "pca_model.fit(X_train)\n",
    "X_pca_train = pca_model.transform(X_train)\n",
    "X_pca_test = pca_model.transform(X_test)\n",
    "plot(X_pca_train,y_train)\n",
    "plot(X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(k)**  Entrene un modelo de Regresión Logística variando el parámetro de regularización $C$ construyendo un gráfico resumen del error en función de este hiper-parámetro. Además entrene una Máquina de Soporte Vectorial (SVM) con kernel lineal, variando el hiper-parámetro de regularizacion $C$ en el mismo rango que para la Regresión Logística, construyendo el mismo gráfico resumen. Compare.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def LR_SVM(X_train,y_train,X_test,y_test):\n",
    "    mse_LR_train = []\n",
    "    mse_LR_test = []\n",
    "\n",
    "    mse_SVM_train = []\n",
    "    mse_SVM_test = []\n",
    "\n",
    "    Ks = []\n",
    "    Ks_ticks = [0.0001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "    for i, Cs in enumerate((0.0001,0.01,0.1,1,10,100,1000)):\n",
    "\n",
    "        model_LR = LR(C=Cs)\n",
    "        model_SVM = SVM(C=Cs)\n",
    "\n",
    "        model_LR.fit(X_train, y_train)\n",
    "        model_SVM.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_LR_train = model_LR.predict(X_train)\n",
    "        y_pred_LR_test = model_LR.predict(X_test)\n",
    "\n",
    "        y_pred_SVM_train = model_SVM.predict(X_train)\n",
    "        y_pred_SVM_test = model_SVM.predict(X_test)\n",
    "\n",
    "        mse_LR_train.append(1-accuracy_score(y_train, y_pred_LR_train))\n",
    "        mse_LR_test.append(1-accuracy_score(y_test, y_pred_LR_test))\n",
    "\n",
    "        mse_SVM_train.append(1-accuracy_score(y_train, y_pred_SVM_train))\n",
    "        mse_SVM_test.append(1-accuracy_score(y_test, y_pred_SVM_test))\n",
    "\n",
    "        Ks.append(i+1)\n",
    "\n",
    "        #print(\"Miss Classification Loss for LR: %f\"%(1-accuracy_score(y_train, y_pred_LR)))\n",
    "        #print(\"Miss Classification Loss for SVM: %f\"%(1-accuracy_score(y_train, y_pred_SVM)))\n",
    "\n",
    "        #print(\"C=%.5f\" % Cs)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    LR_plot = plt.subplot(2,1,1)\n",
    "    LR_plot.plot(Ks, mse_LR_train, label='LR Training Misclassification Error')\n",
    "    LR_plot.plot(Ks, mse_LR_test, label='LR Testing Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.xlabel('Parámetro Regularizador $C$')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.axis([1,6,0.0, 0.7])\n",
    "    plt.xticks(Ks, Ks_ticks)\n",
    "\n",
    "    SVM_plot = plt.subplot(2,1,2)    \n",
    "    SVM_plot.plot(Ks, mse_SVM_train, label = 'SVM Training Misclassification Error')\n",
    "    SVM_plot.plot(Ks, mse_SVM_test, label = 'SVM Test Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.xlabel('Parámetro Regularizador $C$')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.axis([1,6,0.0, 0.7])\n",
    "    plt.xticks(Ks, Ks_ticks)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.xlabel('Parametro Regularización $C$')\n",
    "    #plt.ylabel('Miss Classification Error')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "LR_SVM(X_pca_train,y_train,X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "Se puede apreciar a través de los gráficos que el método de Regesión Logística empieza con un menor error de entrenamiento y de test cuando el parámetro de regularización $C$ es $0.0001$ en comparación con la SVM, pero a medida de que $C$ aumenta, el parámetro de regularización $C$ regulariza de mejor manera los datos de entrenamiento a través de SVM que de LR, pero regulariza de mejor menera el conjunto de test en LR que en SVM, cuando  $C \\rightarrow \\infty$. El menor error alcanzado por LR es aproximadamente de $0.4$ en ambos conjuntos de datos, es más, sí $C \\rightarrow \\infty$, ambos errores convergen a $0.4$. Por otro lado, en SVM el menor error en el conjunto de test converge a 0 sí  $C \\rightarrow \\infty$, en cambio con el conjunto de test, su menor error es aproximadamente de $0.45$ con $C = 1$. <br>\n",
    "Luego de este análisis, se puede afirmar que para este conjunto de datos de entrenamiento, es mejor usar SVM con un parámetro de regularización $C$ grande para obtener el menor error posible y utilizar LR con un parámetro de regularización $C$ grande para obtener el menor error posible sobre el conjunto de datos de test.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(l)**  Entrene un Arbol de Decisión, con la configuración que estime conveniente, variando el hiper-parámetro regularizador _max depth_, construyendo un gráfico resumen del error en función de este parámetro.\n",
    "Compare con los modelos anteriores.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "def DTC_plot(X_pca_train,y_train,X_pca_test,y_test):\n",
    "\n",
    "    Depths = range(1,30)\n",
    "\n",
    "    mse_DTC_train = []\n",
    "    mse_DTC_test = []\n",
    "\n",
    "    for i in Depths:\n",
    "        model_DTC = DTC(max_depth = i)\n",
    "\n",
    "        model_DTC.fit(X_pca_train, y_train)\n",
    "\n",
    "        y_pred_DTC_train = model_DTC.predict(X_pca_train)\n",
    "        y_pred_DTC_test = model_DTC.predict(X_pca_test)\n",
    "\n",
    "        mse_DTC_train.append(1-accuracy_score(y_train, y_pred_DTC_train))\n",
    "        mse_DTC_test.append(1-accuracy_score(y_test, y_pred_DTC_test))\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    DTC_plot = plt.subplot(1,1,1)\n",
    "    DTC_plot.plot(Depths, mse_DTC_train, label='Training Misclassification Error')\n",
    "    DTC_plot.plot(Depths, mse_DTC_test, label='Testing Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.xlabel('Depth DTC')\n",
    "    plt.axis([0,30,0.0, 0.7])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "DTC_plot(X_pca_train,y_train,X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    En comparación con los resultados anteriores, es posible ver que LR sigue manteniendo superioridad sobre el conjuntos de datos de test, ya que obtiene el menor error entre los 3 métodos. En el caso del conjunto de entrenamiento, el DTC converge a 0 con mayor rápidez que la SVM con un parámetro regularizador $C$ grande. Además, se tiene la ventaja que el DTC es más simple de implementar y gastas menos recursos que la SVM (dependiendo de la profundidad del árbol).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(m)**  Experimente con diferentes dimensiones $d$ para la proyección de PCA con el propósito de obtener un\n",
    "modelo con menor error. Construya una tabla o gráfico resumen.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def plots_MCE_PCA(D, C_LR, C_SVM):\n",
    "    mse_LR_train = []\n",
    "    mse_LR_test = []\n",
    "    mse_SVM_train = []\n",
    "    mse_SVM_test = []\n",
    "\n",
    "    model_LR = LR(C=1000)\n",
    "    model_SVM = SVM(C=1)\n",
    "\n",
    "    dim = np.arange(1,D)\n",
    "\n",
    "    for d in range(1,D):\n",
    "\n",
    "        pca_model = PCA(n_components=d)\n",
    "        pca_model.fit(X_train)\n",
    "\n",
    "        X_pca_train = pca_model.transform(X_train)\n",
    "        X_pca_test = pca_model.transform(X_test)\n",
    "\n",
    "        model_LR.fit(X_pca_train, y_train)\n",
    "        model_SVM.fit(X_pca_train, y_train)\n",
    "\n",
    "        y_pred_LR_train = model_LR.predict(X_pca_train)\n",
    "        y_pred_LR_test = model_LR.predict(X_pca_test)\n",
    "\n",
    "        y_pred_SVM_train = model_SVM.predict(X_pca_train)\n",
    "        y_pred_SVM_test = model_SVM.predict(X_pca_test)\n",
    "\n",
    "        mse_LR_train.append(1-accuracy_score(y_train, y_pred_LR_train))\n",
    "        mse_LR_test.append(1-accuracy_score(y_test, y_pred_LR_test))\n",
    "        mse_SVM_train.append(1-accuracy_score(y_train, y_pred_SVM_train))\n",
    "        mse_SVM_test.append(1-accuracy_score(y_test, y_pred_SVM_test))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    LR_plot = plt.subplot(2,1,1)\n",
    "    LR_plot.plot(dim, mse_LR_train, label='LR Training Misclassification Error')\n",
    "    LR_plot.plot(dim, mse_LR_test, label='LR Testing Misclassification Error')\n",
    "    plt.legend(loc='center right')\n",
    "    plt.xlabel('Dimensiones')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "\n",
    "    SVM_plot = plt.subplot(2,1,2)    \n",
    "    SVM_plot.plot(dim, mse_SVM_train, label = 'SVM Training Misclassification Error')\n",
    "    SVM_plot.plot(dim, mse_SVM_test, label = 'SVM Test Misclassification Error')\n",
    "    plt.legend(loc='center right')\n",
    "    plt.xlabel('Dimensiones')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.xlabel('Parametro Regularización $C$')\n",
    "    #plt.ylabel('Miss Classification Error')\n",
    "\n",
    "    plt.show()  \n",
    "\n",
    "widgets.interact(plots_MCE_PCA,D = (1,20),C_LR = (1,1000,1),C_SVM=(1,1000,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(n)**  Realice otra reducción de dimensionalidad ahora a través de la técnica LDA, para representar los datos en $d = 2$ dimensiones. Recuerde que sólo se debe ajustar con el conjunto de entrenamiento, si se muestra un warning explique el porqué. Visualice apropiadamente la proyección en $2$ dimensiones.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "model_lda = LDA(n_components=2)\n",
    "model_lda.fit(X_train,y_train)\n",
    "X_lda_train = model_lda.transform(X_train)\n",
    "X_lda_test = model_lda.transform(X_test)\n",
    "\n",
    "plot(X_lda_train, y_train)\n",
    "#plot(X_pca_test, y_test)\n",
    "\n",
    "#y_pred_lda = model_lda.predict(X_pca_train)\n",
    "\n",
    "#visualize_border(model_lda, X_train, y_train,\"LDA\")\n",
    "#print(X_pca_train)\n",
    "#https://stats.stackexchange.com/questions/29385/collinear-variables-in-multiclass-lda-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   Aparece un warning para advertir que hay variables o atributos colineales, es decir, que son linealmente dependientes.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(o)**  Con el propósito de encontrar el mejor modelo vuelva a realizar el item k) con el l) en el nuevo espacio generado por la representación según las d dimensiones de la proyección LDA. Esta nueva representación ¿mejora o empeora el desempeño? Explique.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC as SVM #SVC is for classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "\n",
    "\n",
    "model_lda = LDA(n_components=2)\n",
    "model_lda.fit(X_train,y_train)\n",
    "X_lda_train = model_lda.transform(X_train)\n",
    "X_lda_test = model_lda.transform(X_test)\n",
    "\n",
    "print(X_lda_train.shape)\n",
    "print(X_lda_test.shape)\n",
    "\n",
    "def LR_SVM_DTC(X_train,y_train,X_test,y_test):\n",
    "\n",
    "    mse_LR_train = []\n",
    "    mse_LR_test = []\n",
    "\n",
    "    mse_SVM_train = []\n",
    "    mse_SVM_test = []\n",
    "\n",
    "    Ks = []\n",
    "    Ks_ticks = [0.0001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "    for i, Cs in enumerate((0.0001,0.01,0.1,1,10,100,1000)):\n",
    "\n",
    "        model_LR = LR(C=Cs)\n",
    "        model_SVM = SVM(C=Cs)\n",
    "\n",
    "        model_LR.fit(X_train, y_train)\n",
    "        model_SVM.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_LR_train = model_LR.predict(X_train)\n",
    "        y_pred_LR_test = model_LR.predict(X_test)\n",
    "\n",
    "        y_pred_SVM_train = model_SVM.predict(X_train)\n",
    "        y_pred_SVM_test = model_SVM.predict(X_test)\n",
    "\n",
    "        mse_LR_train.append(1-accuracy_score(y_train, y_pred_LR_train))\n",
    "        mse_LR_test.append(1-accuracy_score(y_test, y_pred_LR_test))\n",
    "\n",
    "        mse_SVM_train.append(1-accuracy_score(y_train, y_pred_SVM_train))\n",
    "        mse_SVM_test.append(1-accuracy_score(y_test, y_pred_SVM_test))\n",
    "\n",
    "        Ks.append(i+1)\n",
    "\n",
    "        #print(\"Miss Classification Loss for LR: %f\"%(1-accuracy_score(y_train, y_pred_LR)))\n",
    "        #print(\"Miss Classification Loss for SVM: %f\"%(1-accuracy_score(y_train, y_pred_SVM)))\n",
    "\n",
    "        #print(\"C=%.5f\" % Cs)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    LR_plot = plt.subplot(2,1,1)\n",
    "    LR_plot.plot(Ks, mse_LR_train, label='LR Training Misclassification Error')\n",
    "    LR_plot.plot(Ks, mse_LR_test, label='LR Testing Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.xlabel('Parámetro Regularizador $C$')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.axis([1,6,0.0, 0.7])\n",
    "    plt.xticks(Ks, Ks_ticks)\n",
    "\n",
    "    SVM_plot = plt.subplot(2,1,2)    \n",
    "    SVM_plot.plot(Ks, mse_SVM_train, label = 'SVM Training Misclassification Error')\n",
    "    SVM_plot.plot(Ks, mse_SVM_test, label = 'SVM Test Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.xlabel('Parámetro Regularizador $C$')\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.axis([1,6,0.0, 0.7])\n",
    "    plt.xticks(Ks, Ks_ticks)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #plt.xlabel('Parametro Regularización $C$')\n",
    "    #plt.ylabel('Miss Classification Error')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    Depths = range(1,30)\n",
    "\n",
    "    mse_DTC_train = []\n",
    "    mse_DTC_test = []\n",
    "\n",
    "    for i in Depths:\n",
    "        model_DTC = DTC(max_depth = i)\n",
    "\n",
    "        model_DTC.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_DTC_train = model_DTC.predict(X_train)\n",
    "        y_pred_DTC_test = model_DTC.predict(X_test)\n",
    "\n",
    "        mse_DTC_train.append(1-accuracy_score(y_train, y_pred_DTC_train))\n",
    "        mse_DTC_test.append(1-accuracy_score(y_test, y_pred_DTC_test))\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    DTC_plot = plt.subplot(1,1,1)\n",
    "    DTC_plot.plot(Depths, mse_DTC_train, label='Training Misclassification Error')\n",
    "    DTC_plot.plot(Depths, mse_DTC_test, label='Testing Misclassification Error')\n",
    "    plt.legend(loc=3)\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.xlabel('Depth')\n",
    "    plt.axis([0,30,0.0, 0.7])\n",
    "\n",
    "    plt.show()\n",
    "LR_SVM_DTC(X_lda_train,y_train,X_lda_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   <ul>\n",
    "       <li>LR:\n",
    "           <ul>\n",
    "             <li>Error de Entrenamiento: Al empezar mejora de un 0.55 a un 0.05 aproximadamente y se mantiene constante, logrando una notable diferencia con respecto al error de 0.4 obtenido anteriormente en el punto k).</li>\n",
    "             <li>Error de Test: Al empezar mejora de un 0.6 a un 0.3 aproximadamente y se mantiene constante en 0.3, obteniendo un mejor resultado que el anterior, ya que este convergia a 0.4.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "       <li>SVM:\n",
    "            <ul>\n",
    "             <li>Error de Entrenamiento: empieza con el mismo error pero converge más rápido a 0 a medida de que el parámetro regularizador $C$ aumenta. Anteriormente se tenía que con un $C > 1000$ convergía a 0, en cambio ahora con los datos ajustados con LDA, basta con un $C = 30$.</li>\n",
    "             <li>Error de Test: al igual que en el error de entrenamiento, empieza con el mismo error pero logra un error menor a medida de que el parámetro regularizador $C$ crece. En este caso, el mínimo error de test es de un 0.4, una décima más grande que el error anterior.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "       <li>DTC:\n",
    "            <ul>\n",
    "             <li>Error de Entrenamiento: converge de manera más rápida, ya que con profundida igual a 5, el error de entrenamiento alcanza el valor 0.</li>\n",
    "             <li>Error de Test: En este caso, aumenta el error de test en comparación en el punto l) en aproximadamente 0.05 décimas.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "   </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(p)**  Intente mejorar el desempeño de los algoritmos ya entrenados. Diseñe ahora sus propias características (_feature crafting_) a partir de los datos brutos (secuencia de amplitudes), puede inspirarse en otros trabajos [6] [7] si desea.\n",
    "</p>\n",
    "\n",
    "- [6]  https://www.kaggle.com/primaryobjects/voicegender/data\n",
    "- [7]  Gamit, M. R., Dhameliya, P. K., & Bhatt, N. S. (2015). Classification Techniques for Speech Recognition: A Review. vol, 5, 58-63.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import skew\n",
    "from scipy import stats\n",
    "from statistics import mode, StatisticsError\n",
    "\n",
    "def centroide(X,Y):\n",
    "    return [sum(X)/len(X),sum(Y)/len(Y)]\n",
    "\n",
    "df = pd.read_csv('./heartbeat-sounds/set_a.csv')\n",
    "df.drop(['dataset','sublabel',],axis=1,inplace=True)\n",
    "new_df =pd.DataFrame({'file_name' : df['fname'].apply(clean_filename,string='Aunlabelledtest')})\n",
    "new_df['time_series'] = new_df['file_name'].apply(load_wav_file, path='./heartbeat-sounds/set_a/')\n",
    "new_df['len_series'] = new_df['time_series'].apply(len)\n",
    "new_df['time_series'] = new_df['time_series'].apply(padd_zeros,length=max(new_df['len_series']))\n",
    "new_labels =[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "             1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2,\n",
    "             2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
    "             1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 0,\n",
    "             2, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 0,\n",
    "             0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "labels = ['artifact','normal/extrahls', 'murmur']\n",
    "new_df['target'] = [labels[i] for i in new_labels]\n",
    "new_df[\"target\"] = new_df[\"target\"].astype('category')\n",
    "cat_columns = new_df.select_dtypes(['category']).columns\n",
    "new_df[cat_columns] = new_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "new_df.head()\n",
    "new_df = new_df.sample(frac=1,random_state=44)\n",
    "X = np.stack(new_df['time_series'].values, axis=0)\n",
    "y = new_df.target.values\n",
    "data = np.zeros((X.shape[0],13))\n",
    "\n",
    "for i in range(0,X.shape[0]):\n",
    "    data[i][0] = np.mean(X[i])\n",
    "    data[i][1] = np.median(X[i])\n",
    "    data[i][2] = np.std(X[i])\n",
    "    data[i][3] = (pd.Series(X[i])).quantile(0.25)\n",
    "    data[i][4] = (pd.Series(X[i])).quantile(0.75)\n",
    "    data[i][5] = iqr(X[i])\n",
    "    data[i][6] = skew(X[i])\n",
    "    try:\n",
    "        data[i][7] = mode(X[i])\n",
    "    except StatisticsError:\n",
    "        data[i][7] = 0\n",
    "        \n",
    "    data[i][8] = min(X[i])    \n",
    "    data[i][9] = max(X[i])\n",
    "    data[i][10] = np.mean(np.fft.fftfreq(len(X[i])))\n",
    "    if i < X.shape[0]-1:\n",
    "        data[i][11] = centroide(X[i],X[i+1])[0]\n",
    "        data[i][12] = centroide(X[i],X[i+1])[1]\n",
    "        \n",
    "\n",
    "#https://www.kaggle.com/primaryobjects/voicegender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Mean\",\"Median\", \"Std\",\"Q25\",\"Q75\",\"IQR\",\"Skew\",\"Mode\",\"Min\",\"Max\",\"MFreq\",\"Centroide i\", \"Centroide i+1\"]\n",
    "df2 = pd.DataFrame(data = data, columns = headers)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fourier = np.abs(np.fft.fft(data))\n",
    "data_resampled = []\n",
    "for i in range(X_fourier.shape[0]):\n",
    "    sequence = data_fourier[i,:].copy()\n",
    "    resampled_sequence = signal.resample(sequence, 100000)\n",
    "    data_resampled.append(resampled_sequence)\n",
    "\n",
    "data_resampled = np.array(data_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.25, random_state=43)\n",
    "print(\"El set de entrenamiento tiene \" + str(len(X_train)) \n",
    "      + \" y el set de test tiene \" + str(len(X_test)) \n",
    "      + \" datos, y ambos tienen 10000 clases\")\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=2)\n",
    "pca_model.fit(X_train,y_train)\n",
    "X_pca_train = pca_model.transform(X_train)\n",
    "X_pca_test = pca_model.transform(X_test)\n",
    "plot(X_pca_train,y_train)\n",
    "plot(X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_SVM_DTC(X_pca_train,y_train,X_pca_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   <ul>\n",
    "       <li>LR:\n",
    "           <ul>\n",
    "             <li>Error de Test: Se obtiene un menor error de test que con los datos anteriormente seleccionados, al ocupar un parámetro regularizador de 0.0001 se obtiene el menor error de 0.3.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "       <li>SVM:\n",
    "            <ul>\n",
    "             <li>Error de Test: No se mejora mucho en comparación con los datos anteriores, pero se obtiene un error de 0.4 aproximadamente cuando se ocupa un parámetro regularizador $10 <= C <= 100$.</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "       <li>DTC:\n",
    "            <ul>\n",
    "             <li>Error de Test: Se obtiene el mínimo error de test, un 0.3</li>\n",
    "           </ul>\n",
    "       </li>\n",
    "   </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(n_components=2)\n",
    "lda_model.fit(X_train,y_train)\n",
    "X_lda_train = lda_model.transform(X_train)\n",
    "X_lda_test = lda_model.transform(X_test)\n",
    "plot(X_lda_train,y_train)\n",
    "plot(X_lda_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_SVM_DTC(X_lda_train,y_train,X_lda_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Análisis de emociones en tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)**  Construya un dataframe con los datos a analizar. Determine cuántas clases existen, cuántos registros por clase y describa el _dataset_.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('./text_emotion.csv')\n",
    "df_copia = df.copy()\n",
    "valores = df.sentiment.value_counts()\n",
    "def plot_valores(valores):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    location = np.arange(len(valores))\n",
    "    width = 0.45\n",
    "    uno = ax.bar(location, valores.values(),  width, align='center', color=\"#F5A9F2\")\n",
    "    ax.set_xticks(location)\n",
    "    ax.set_xticklabels(list(valores.keys()), fontsize=15)\n",
    "    ax.set_title('Data Distribution')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plot_valores(dict(valores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Sentimiento | Registro |\n",
    "|:-----------:|:--------:|\n",
    "|   Neutral   |   8638   |\n",
    "|    Worry    |   8459   |\n",
    "|  Happiness  |   5209   |\n",
    "|   Sadness   |   5165   |\n",
    "|     Love    |   3842   |\n",
    "|   Surprise  |   2187   |\n",
    "|     Fun     |   1776   |\n",
    "|    Relief   |   1526   |\n",
    "|     Hate    |   1323   |\n",
    "|    Empty    |    827   |\n",
    "|  Enthusiasm |    759   |\n",
    "|   Boredom   |    179   |\n",
    "|    Anger    |    110   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)**  Construya un conjunto de entrenamiento y otro de pruebas, a través de una máscara aleatoria, para verificar los resultados de los algoritmos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(22)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_resampled, y, test_size=0.25, random_state=43)\n",
    "\n",
    "#print(df['content'][2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)**  Implemente y explique un pre-procesamiento para los tweets para dejarlos en un formato estándarizado\n",
    "en el cual se podrán trabajar.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['tweet_id','author'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\[\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "#print(preprocess(df['content'][0]))\n",
    "\n",
    "#df['content'] = df['content'].str\n",
    "df['content'] = df['content'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"rt\",\"via\",\"a\",\"able\",\"about\",\"across\",\"after\",\"all\",\"almost\",\"also\",\"am\",\"among\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\"be\",\"because\",\"been\",\"but\",\"by\",\"can\",\"cannot\",\"could\",\"dear\",\"did\",\"do\",\"does\",\"either\",\"else\",\"ever\",\"every\",\"for\",\"from\",\"get\",\"got\",\"had\",\"has\",\"have\",\"he\",\"her\",\"hers\",\"him\",\"his\",\"how\",\"however\",\"i\",\"if\",\"in\",\"into\",\"is\",\"it\",\"its\",\"just\",\"least\",\"let\",\"like\",\"likely\",\"may\",\"me\",\"might\",\"most\",\"must\",\"my\",\"neither\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"often\",\"on\",\"only\",\"or\",\"other\",\"our\",\"own\",\"rather\",\"said\",\"say\",\"says\",\"she\",\"should\",\"since\",\"so\",\"some\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"there\",\"these\",\"they\",\"this\",\"tis\",\"to\",\"too\",\"twas\",\"us\",\"wants\",\"was\",\"we\",\"were\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"would\",\"yet\",\"you\",\"your\"]\n",
    "stop = stopwords.words('english')\n",
    "result = list(set(stop)|set(stop_words)|set(list('abcdefghijklmnopqrstuvwxyz')) | set(list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')) | set(list('0123456789')))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda x: [item for item in x if item not in result])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)**  Haga una reducción binaria al problema, para trabajarlo como un problema de clasificación de dos clases.\n",
    "Para esto, agrupe las distintas emociones como positivas y negativas (defina un criterio), se recomienda\n",
    "codificar las clases como +1 y −1 respectivamente. Recuerde tener presente que el desbalanceo de los\n",
    "datos puede afectar considerablemente al modelo.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uno = ['happiness', 'love', 'fun', 'relief', 'enthusiasm']\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x in uno else -1)\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)**  Para construir un clasificador que determine automáticamente la polaridad de un trozo de texto, será\n",
    "necesario representar los tweets $\\{t_i\\}_{i=1}^{n}$ disponibles como vectores de características (features). El tipo\n",
    "de características más utilizado consiste en contar cuántas veces aparecen ciertos términos/palabras en\n",
    "el texto. Para esto, es necesario un vocabulario que, por lo general, se construye mediante la unión de\n",
    "todas las palabras que se observen en los tweets.\n",
    "\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\">\n",
    "Se recomienda utilizar las librerías ofrecidas por sklearn de feature extraction in text [12] (_CountVectorizer_ y _TfidfVectorizer_). Recuerde realizar el ajuste (_fit_) únicamente con el conjunto de entrenamiento, para luego transformar el conjunto de pruebas (con el método transform).\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "- [12] http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "X = df['content'].values\n",
    "y = df.sentiment.values\n",
    "XX = df_copia['content'].values\n",
    "yy = y.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "XX_train, XX_test, yy_train, yy_test = train_test_split(XX, yy, test_size=0.3, random_state=22)\n",
    " \n",
    "def fake(x):\n",
    "    return x\n",
    "\n",
    "#vectorizer = TfidfVectorizer(tokenizer=fake, preprocessor=fake, lowercase=False)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=fake, preprocessor=fake, lowercase=False)\n",
    "#CV_fit = CV_model.fit_transform(X_train.tostring())\n",
    "#X_CV_train = CV_model.transform(X_train)\n",
    "\n",
    "X_CV_train = vectorizer.fit_transform(X_train)\n",
    "X_CV_test = vectorizer.transform(X_test)\n",
    "\n",
    "type(X_CV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_model.fit(X_train.toarray(), df_train['sentiment'])\n",
    "\n",
    "qda_score_train = qda_model.score(X_train.toarray(), df_train['sentiment'])\n",
    "qda_score_test = qda_model.score(X_test.toarray(), df_test['sentiment'])\n",
    "\n",
    "print(\"NB Train score %f\"%qda_score_train)\n",
    "print(\"NB Test score %f\"%qda_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train.toarray(), df_train['sentiment'])\n",
    "\n",
    "lda_score_train = lda_model.score(X_train, df_train['sentiment'])\n",
    "lda_score_test = lda_model.score(X_test, df_test['sentiment'])\n",
    "\n",
    "print(\"NB Train score %f\"%lda_score_train)\n",
    "print(\"NB Test score %f\"%lda_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)**  Entrene y compare al menos 5 de los diferentes clasificadores vistos en clases para clasificación binaria (por ejemplo: Navie Bayes, Multinomial Naive Bayes, LDA, QDA, Regresión logística, SVM y Arboles de decisión) sobre el conjunto de entrenamiento verificando su desempeño sobre ambos conjuntos (entrenamiento y de pruebas), construyendo un gráfico resumen del error de éstos.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_comparation(ScoreTrain, ScoreTest):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "    location = np.arange(len(ScoreTrain))\n",
    "    width = 0.45\n",
    "    \n",
    "    uno = ax.barh(location, ScoreTrain.values(),  width, align='center', color=\"#F5A9F2\")\n",
    "    dos = ax.barh(location + width, ScoreTest.values(), width, align='center', color=\"#A9F5F2\")\n",
    "    ax.set_yticks(location + width/2)\n",
    "    ax.set_yticklabels((\"NB\", \"DTC\", \"LR\", \"KNC\", \"MLPC\"))\n",
    "    \n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title('Train and Test Score')\n",
    "    ax.legend((uno[0], dos[0]), ('Train', 'Test'), bbox_to_anchor=(1,0.6))\n",
    "    #autolabel(uno)\n",
    "    #autolabel(dos)\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "ScoreTrain = {}\n",
    "ScoreTest = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_model = BernoulliNB()\n",
    "nb_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"NB Train\"] = nb_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"NB Test\"] = nb_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"NB Train score %f\"%ScoreTrain[\"NB Train\"])\n",
    "print(\"NB Test score %f\"%ScoreTest[\"NB Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "DTC_model = DTC()\n",
    "DTC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"DTC Train\"] = DTC_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"DTC Test\"] = DTC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"DTC Train score %f\"%ScoreTrain[\"DTC Train\"])\n",
    "print(\"DTC Test score %f\"%ScoreTest[\"DTC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "LR_model = LR()\n",
    "LR_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"LR Train\"] = LR_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"LR Test\"] = LR_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"LR Train score %f\"%ScoreTrain[\"LR Train\"])\n",
    "print(\"LR Test score %f\"%ScoreTest[\"LR Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "KNC_model = KNC()\n",
    "KNC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"KNC Train\"] = KNC_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"KNC Test\"] = KNC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"KNC Train score %f\"%ScoreTrain[\"KNC Train\"])\n",
    "print(\"KNC Test score %f\"%ScoreTest[\"KNC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "MLPC_model = MLPC()\n",
    "MLPC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain[\"MLPC Train\"] = MLPC_model.score(X_CV_train, y_train)\n",
    "ScoreTest[\"MLPC Test\"] = MLPC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"MLPC Train score %f\"%ScoreTrain[\"MLPC Train\"])-\n",
    "print(\"MLPC Test score %f\"%ScoreTest[\"MLPC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparation(ScoreTrain, ScoreTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(g)**  Utilice y explique las métricas que calcula la función classification report de la librería sklearn. En base\n",
    "a las distintas métricas calculadas ¿Cuáles clasificadores son los que mejor se comportan?\n",
    "</p>\n",
    "\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    _Classification Report_ entrega las metricas principales de los clasificadores, es decir, un resumen de la precisión, el recall y el valor F.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    En este contexto se denomina precisión (denominado igualmente valor positivo predicho) como a la fracción de instancias recuperadas que son relevantes, mientras recall (denominado igualmente sensibilidad o exhaustividad) es la fracción de instancias relevantes que han sido recuperadas.\n",
    "</p>\n",
    "\n",
    "<img src=\"precisionrecall.png\">\n",
    "\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    El valor F (en ingles F-score) es la medida de precisión que tiene un test. Considera el valor de precisión $p$ y el de recall $r$ de test para calcular el valor.\n",
    "</p>\n",
    "\n",
    "<img src=\"F1.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def score_the_model(model,x,y,xt,yt):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print(\"___________________________________________________________________________\")\n",
    "    print(\"Training Accuracy: %f\"%(acc_tr))\n",
    "    print(\"Test Accuracy: %f\"%(acc_test))\n",
    "    print(\"Detailed Analysis Testing Results ...\")\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))\n",
    "    \n",
    "score_the_model(nb_model, X_CV_train, y_train, X_CV_test, y_test)\n",
    "score_the_model(DTC_model, X_CV_train, y_train, X_CV_test, y_test)\n",
    "score_the_model(LR_model, X_CV_train, y_train, X_CV_test, y_test)\n",
    "score_the_model(KNC_model, X_CV_train, y_train, X_CV_test, y_test)\n",
    "score_the_model(MLPC_model, X_CV_train, y_train, X_CV_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(h)**  [Opcional] Visualice las predicciones de algún modelo generativo (probabilístico) definido anteriormente, tomando un subconjunto aleatorio de tweets de pruebas y explorando las probabilidades que asigna el clasificador a cada clase.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test_pred = LR_model.predict_proba(X_CV_test)\n",
    "#print(test_pred)\n",
    "random.seed(22)\n",
    "spl = random.sample(range(len(test_pred)), 15)\n",
    "\n",
    "\n",
    "print(\"Negative \\t Positive \\t Tweet\")\n",
    "for i in spl:\n",
    "    print(test_pred[i],\"\\t\",XX_test[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(i)**  Ahora deberá extender el problema a las múltiples clases que tiene presente (las distintas emociones),\n",
    "es decir, su trabajo será el de predecir una de las distintas emociones de cada _tweet_. Para esto utilice el\n",
    "mismo pre-procesamiento realizado en el punto c) y las características generadas mediante las técnicas\n",
    "en el punto e). Recuerde que tendrá que codificar las distintas clases como valores numéricos enteros.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('./text_emotion.csv')\n",
    "df.shape\n",
    "np.random.seed(22)\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "df.drop(['tweet_id','author'],axis=1,inplace=True)\n",
    "df['content'] = df['content'].apply(preprocess)\n",
    "df['content'] = df['content'].apply(lambda x: [item for item in x if item not in result])\n",
    "\n",
    "#Sentimientos\n",
    "#Neutral, Worry, Happiness, Sadness, Love, Surprise, Fun, Relief, Hate, Empty, Enthusiasm, Boredom, Anger\n",
    "\n",
    "#Propuesta orden de sentimiento\n",
    "#Happiness, Fun, Love, Surprise, Enthusiasm, Relief, Neutral, Empty,  Boredom, Worry, Sadness, Hate, Anger\n",
    "#\"Happiness\", \"Fun\", \"Love\", \"Surprise\", \"Enthusiasm\", \"Relief\", \"Neutral\", \"Empty\", \"Boredom\", \"Worry\", \"Sadness\", \"Hate\", \"Anger\"\n",
    "\n",
    "emotions = [\"Happiness\", \"Love\", \"Fun\", \"Surprise\", \"Enthusiasm\", \"Relief\", \"Neutral\", \"Empty\", \"Boredom\", \"Worry\", \"Sadness\", \"Anger\", \"Hate\"]\n",
    "\n",
    "emotions =  [x.lower() for x in reversed(emotions)]\n",
    "\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: emotions.index(x)-6)\n",
    "df.head()\n",
    "\n",
    "X = df['content'].values\n",
    "y = df.sentiment.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "\n",
    "def fake(x):\n",
    "    return x\n",
    "\n",
    "#vectorizer = TfidfVectorizer(tokenizer=fake, preprocessor=fake, lowercase=False)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=fake, preprocessor=fake, lowercase=False)\n",
    "\n",
    "X_CV_train = vectorizer.fit_transform(X_train)\n",
    "X_CV_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(j)**  Utilice los clasificadores que son extendidos por defecto a múltiples clases para detectar emociones en\n",
    "cada _tweet_, muestre sus desempeños a través del error de pruebas en un gráfico resumen.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScoreTrain_M = {}\n",
    "ScoreTest_M = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "DTC_model = DTC()\n",
    "DTC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"DTC Train\"] = DTC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"DTC Test\"] = DTC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"DTC Train score %f\"%ScoreTrain_M[\"DTC Train\"])\n",
    "print(\"DTC Test score %f\"%ScoreTest_M[\"DTC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "KNC_model = KNC()\n",
    "KNC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"KNC Train\"] = KNC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"KNC Test\"] = KNC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"KNC Train score %f\"%ScoreTrain_M[\"KNC Train\"])\n",
    "print(\"KNC Test score %f\"%ScoreTest_M[\"KNC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "MLPC_model = MLPC()\n",
    "MLPC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"MLPC Train\"] = MLPC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"MLPC Test\"] = MLPC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"MLPC Train score %f\"%ScoreTrain_M[\"MLPC Train\"])\n",
    "print(\"MLPC Test score %f\"%ScoreTest_M[\"MLPC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "MNB_model = MNB()\n",
    "MNB_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"MNB Train\"] = MNB_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"MNB Test\"] = MNB_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"MNB Train score %f\"%ScoreTrain_M[\"MNB Train\"])\n",
    "print(\"MNB Test score %f\"%ScoreTest_M[\"MNB Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(k)**  Utilice clasificadores binarios que pueden ser extendidos a través de otras técnicas, tal como One vs\n",
    "One y One vs All/Rest [14]\n",
    "</p>\n",
    "\n",
    "- [14]  http://scikit-learn.org/stable/modules/classes.html#module-sklearn.multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier as ORC\n",
    "ORC_model = ORC(MNB(), -1)\n",
    "ORC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"ORC Train\"] = ORC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"ORC Test\"] = ORC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"ORC Train score %f\"%ScoreTrain_M[\"ORC Train\"])\n",
    "print(\"ORC Test score %f\"%ScoreTest_M[\"ORC Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier as OOC\n",
    "OOC_model = OOC(MNB(), -1)\n",
    "OOC_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"OOC Train\"] = OOC_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"OOC Test\"] = OOC_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"OOC Train score %f\"%ScoreTrain_M[\"OOC Train\"])\n",
    "print(\"OOC Test score %f\"%ScoreTest_M[\"OOC Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(l)**  Para el caso de la Regresión Logística compare sus dos métodos para ser extendidos a múltiples clases.\n",
    "Uno a través de One vs Rest y otro definiendo que la variable a predecir se distribuye Multinomial.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "LR_model = LR(multi_class='ovr')\n",
    "LR_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"LR OvR Train\"] = LR_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"LR OvR Test\"] = LR_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"LR Train score %f\"%ScoreTrain_M[\"LR OvR Train\"])\n",
    "print(\"LR Test score %f\"%ScoreTest_M[\"LR OvR Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LR(multi_class='multinomial', solver=\"newton-cg\")\n",
    "#Currently the ‘multinomial’ option is supported only by the ‘lbfgs’ and ‘newton-cg’ solvers.\n",
    "LR_model.fit(X_CV_train, y_train)\n",
    "\n",
    "ScoreTrain_M[\"LR Multi Train\"] = LR_model.score(X_CV_train, y_train)\n",
    "ScoreTest_M[\"LR Multi Test\"] = LR_model.score(X_CV_test, y_test)\n",
    "\n",
    "print(\"LR Train score %f\"%ScoreTrain_M[\"LR Multi Train\"])\n",
    "print(\"LR Test score %f\"%ScoreTest_M[\"LR Multi Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(m)**  Compare los resultados entre los clasificadores extendidos por defecto y los binarios que son extendidos mediante otras técnicas, construya una tabla o gráfico resumen. Los clasificadores que mejor se comportan en el caso binario ¿Siguen teniendo ese desempeño en múltiples clases?\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_comparation_M(ScoreTrain, ScoreTest):\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "    location = np.arange(len(ScoreTrain))\n",
    "    width = 0.45\n",
    "    \n",
    "    uno = ax.barh(location, ScoreTrain.values(),  width, align='center', color=\"#F5A9F2\")\n",
    "    dos = ax.barh(location + width, ScoreTest.values(), width, align='center', color=\"#A9F5F2\")\n",
    "    ax.set_yticks(location + width/2)\n",
    "    ax.set_yticklabels((\"DTC\", \"KNC\", \"MLPC\", \"MNB\", \"ORC\", \"OOC\", \"LR OvR\", \"LR Multi\"))\n",
    "    \n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title('Train and Test Score')\n",
    "    ax.legend((uno[0], dos[0]), ('Train', 'Test'), bbox_to_anchor=(1,0.6))\n",
    "    #autolabel(uno)\n",
    "    #autolabel(dos)\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plot_comparation_M(ScoreTrain_M, ScoreTest_M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
