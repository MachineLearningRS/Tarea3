{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div><img src=\"escudo_utfsm.gif\" style=\"float:right;height:100px\">\n",
    "<img src=\"IsotipoDIisocolor.png\" style=\"float:left;height:100px\">\n",
    "<center>\n",
    "    <h1> INF-393 - Machine Learning</h1>\n",
    "    <h1> Tarea 3 - Métodos No Lineales </h1>\n",
    "\n",
    "<p>\n",
    "<br><center>_Javier Reyes_<strong> - </strong>_javier.reyes.12@sansano.usm.cl_<strong> - </strong>_201273524-6_ </center>\n",
    "<br><center>_Marco Salinas_<strong> - </strong>_marco.salinas.12@sansano.usm.cl_<strong> - </strong>_201273589-0_ </center>\n",
    "</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1 Small Circle inside Large Circle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)** Escriba una función que genere (aleatoriamente) $n$ datos etiquetados de la forma $\\{(x_1; y_1), ... ,(x_n; y_n)\\}, x_i \\in R^2, y_i \\in \\{0, 1\\}$, con una distribución de probabilidad que re eje la configuración linealmente inseparable que muestra la Fig. 1. Utilice esta función para crear 1000 datos de entrenamiento y 1000 datos de pruebas. Para medir la tendencia de los modelos a sobre-ajuste, agregue un 5% de ruido al dataset, generando x's cercanos a la frontera. Genere un gráfico que muestre datos de entrenamiento y pruebas, identificando cada clase con un color diferente.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def do_circles(n=2000,noisy_n=0.05):\n",
    "    generator = check_random_state(10)\n",
    "    linspace = np.linspace(0, 2 * np.pi, n // 2 + 1)[:-1]\n",
    "    outer_circ_x = np.cos(linspace)\n",
    "    outer_circ_y = np.sin(linspace)\n",
    "    inner_circ_x = outer_circ_x * .3\n",
    "    inner_circ_y = outer_circ_y * .3\n",
    "    X = np.vstack((np.append(outer_circ_x, inner_circ_x),\n",
    "                   np.append(outer_circ_y, inner_circ_y))).T\n",
    "    \n",
    "    y = np.hstack([np.zeros(n // 2, dtype=np.intp),\n",
    "                   np.ones(n // 2, dtype=np.intp)])\n",
    "    X += generator.normal(scale=noisy_n, size=X.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "X_train,Y_train,X_test,Y_test = do_circles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    Para lo que sigue de la actividad utilice la siguiente función para graficar las fronteras de clasificación en\n",
    "base a la probabilidad, definida por un algoritmo, de un ejemplo a pertenecer a una clase en particular.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "def plot_classifier(clf,X_train,Y_train,X_test,Y_test,model_type,lim):\n",
    "    f, axis = plt.subplots(1, 1, sharex='col', sharey='row', figsize=(12, 8))\n",
    "    axis.scatter(X_train[:,0],X_train[:,1],s=30,c=Y_train,zorder=10,cmap=plt.cm.Set3,label=\"train data\")\n",
    "    axis.scatter(X_test[:,0],X_test[:,1],s=20,c=Y_test,zorder=10,cmap='tab20b')\n",
    "    XX, YY = np.mgrid[-2:2:200j, -2:2:200j]\n",
    "    if model_type == 'tree':\n",
    "        Z = clf.predict_proba(np.c_[XX.ravel(), YY.ravel()])[:,0]\n",
    "        axis.set_title(\"Decision Tree Classifier\")\n",
    "    elif model_type != 'tree':\n",
    "        Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "        axis.set_title(model_type)\n",
    "    else: raise ValueError('model type not supported')\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    Zplot = Z >= 0.5\n",
    "    axis.pcolormesh(XX, YY, Zplot ,cmap='Greys')\n",
    "    axis.contour(XX, YY, Z, alpha=1, colors=[\"k\", \"k\", \"k\"], linestyles=[\"--\", \"-\", \"--\"], levels=[-2, 0, 2])\n",
    "    navy_train = mpatches.Patch(color='navy',label='Train data')\n",
    "    lightblue_train = mpatches.Patch(color='lightblue',label='Train data')\n",
    "    yellow_test = mpatches.Patch(color='yellow',label='Test data')\n",
    "    violet_test = mpatches.Patch(color='violet',label='Test data')\n",
    "    axis.legend(handles=[navy_train,lightblue_train,yellow_test,violet_test])\n",
    "    axis.set_xlim(-lim,lim)\n",
    "    axis.set_ylim(-lim,lim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Tree = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "Tree.fit(X_train, Y_train)\n",
    "plot_classifier(Tree,X_train,Y_train,X_test,Y_test,\"tree\",1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C=100, penalty='l2', tol=0.01)\n",
    "LR.fit(X_train, Y_train)\n",
    "plot_classifier(LR,X_train,Y_train,X_test,Y_test,\"Logistic Regression\",1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM\n",
    "model_SVM = SVM(kernel='rbf')\n",
    "model_SVM.fit(X_train,Y_train)\n",
    "plot_classifier(model_SVM,X_train,Y_train,X_test,Y_test,\"SVM\",1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)** Demuestre __experimentalmente__ que una red neuronal artificial correspondiente a 1 sola neurona (i.e. sin capas escondidas) no puede resolver satisfactoriamente el problema. Puede utilizar la función de activación y el método de entrenamiento que prefiera. Sea convincente: por ejemplo, intente modificar los parámetros de la máquina de aprendizaje, reportando métricas que permitan evaluar el desempeño del modelo en el problema con cada cambio efectuado. Adapte también la función plot classifier para que represente gráficamente la solución encontrada por la red neuronal. Describa y explique lo que observa, reportando gráficos de la solución sólo para algunos casos representativos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD,RMSprop,Adam,Adagrad\n",
    "n_h=1\n",
    "Seq_model = Sequential()\n",
    "Seq_model.add(Dense(1, input_dim=X_train.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "Seq_model.add(Dense(n_h, init='uniform', activation='sigmoid'))\n",
    "Seq_model.compile(optimizer=SGD(lr=1, momentum=0.0, decay=0.0, nesterov=True), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])#Stochastic Gradient Descent\n",
    "history_train = Seq_model.fit(X_train, Y_train, epochs=50, batch_size=100, verbose=1, callbacks=[])\n",
    "history_test = Seq_model.fit(X_test, Y_test, epochs=50, batch_size=100, verbose=1,callbacks=[])\n",
    "scores = Seq_model.evaluate(X_test, Y_test)\n",
    "test_acc = scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier(Seq_model,X_train,Y_train,X_test,Y_test,\"Sequential\",1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sequential_plot_loss(loss_train,loss_test):\n",
    "    \n",
    "    x = range(0,len(loss_train))\n",
    "    plt.figure(figsize=(12,6))\n",
    "    DTC_plot = plt.subplot(1,1,1)\n",
    "    DTC_plot.plot(x, loss_train, label='Training Misclassification Error')\n",
    "    DTC_plot.plot(x, loss_test, label='Testing Misclassification Error')\n",
    "    plt.legend(bbox_to_anchor=(1,0.6))\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.title(\"Sequential loss\")\n",
    "    plt.show()\n",
    "    \n",
    "def Sequential_plot_acc(acc_train,acc_test):\n",
    "    \n",
    "    acc_train_score = []\n",
    "    acc_test_score = []\n",
    "    for i in range(0,len(acc_train)):\n",
    "        acc_train_score.append(1-acc_train[i])\n",
    "        acc_test_score.append(1-acc_test[i])\n",
    "    x = range(0,len(acc_train))\n",
    "    plt.figure(figsize=(12,6))\n",
    "    DTC_plot = plt.subplot(1,1,1)\n",
    "    DTC_plot.plot(x, acc_train_score, label='Training Misclassification Error')\n",
    "    DTC_plot.plot(x, acc_test_score, label='Testing Misclassification Error')\n",
    "    plt.legend(bbox_to_anchor=(1,0.6))\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.title(\"Sequential acc\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history_train.history['loss']\n",
    "loss_test = history_test.history['loss']\n",
    "acc_train = history_train.history['acc']\n",
    "acc_test = history_test.history['acc']\n",
    "\n",
    "Sequential_plot_loss(loss_train,loss_test)\n",
    "Sequential_plot_acc(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)** Demuestre **experimentalmente** que una red neuronal artificial con 1 capa escondida puede resolver satisfactoriamente el problema obtenido en (a). Puede utilizar la arquitectura y el método de entrenamiento\n",
    "que prefiera, pero en esta actividad puede optar tranquilamente por usar los hiper-parámetros que se 3 entregan como referencia en el código de ejemplo. Cambie el número de neuronas Nh en la red entre 2 y 32 en potencias de 2, graficando el error de entrenamiento y pruebas como función de Nh. Describa y explique lo que observa. Utilice la función *plot_classifier*, diseñada anteriormente, para construir gráficos de la solución en algunos casos representativos.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD,RMSprop,Adam,Adagrad\n",
    "n_h=32\n",
    "Seq_model = Sequential()\n",
    "Seq_model.add(Dense(n_h, input_dim=X_train.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "Seq_model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "Seq_model.compile(optimizer=SGD(lr=1, momentum=0.0, decay=0.0, nesterov=True), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])#Stochastic Gradient Descent\n",
    "history_train = Seq_model.fit(X_train, Y_train, epochs=50, batch_size=100, verbose=1, callbacks=[])\n",
    "history_test = Seq_model.fit(X_test, Y_test, epochs=50, batch_size=100, verbose=1,callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history_train.history['loss']\n",
    "loss_test = history_test.history['loss']\n",
    "acc_train = history_train.history['acc']\n",
    "acc_test = history_test.history['acc']\n",
    "plot_classifier(Seq_model,X_train,Y_train,X_test,Y_test,\"Sequential\",1.25)\n",
    "Sequential_plot_loss(loss_train,loss_test)\n",
    "Sequential_plot_acc(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n",
    "<p  style=\"text-align: justify;\"> \n",
    "    asdf\n",
    "</p>\n",
    "\n",
    "<p  style=\"text-align: justify;\">\n",
    "    asdf\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)** Demuestre **experimentalmente** que stump (árbol de clasificación de 1 nivel) no puede resolver satisfactoriamente el problema anterior. Puede utilizar el criterio y la función de partición que prefiera. Sea convincente: por ejemplo, intente modificar los parámetros de la máquina, reportando métricas que permitan evaluar el desempeño del modelo en el problema con cada cambio efectuado. Adapte también la función *plot_classifier* para que represente gráficamente la solución encontrada por el árbol. Describa y explique lo que observa, reportando gráficos de la solución sólo para algunos casos representativos.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "clf=Tree(criterion='gini',splitter='best',random_state=0,max_depth=1)\n",
    "clf.fit(X_train,Y_train)\n",
    "acc_test = clf.score(X_test,Y_test)\n",
    "print(\"Test Accuracy = %f\"%acc_test)\n",
    "print(clf.tree_.max_depth)\n",
    "plot_classifier(clf,X_train,Y_train,X_test,Y_test,'tree',1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)** Demuestre experimentalmente que un árbol de clasificación de múltiples niveles puede resolver satisfactoriamente el problema estudiado. Puede utilizar el criterio y la función de partición que prefiera, pero puede optar tranquilamente por usar los hiper-parámetros que se entregan como referencia en el código de ejemplo. Cambie el número de niveles admitidos en el árbol Nt entre 2 y 20, graficando el error de entrenamiento y pruebas como función de Nt. Describa y explique lo que observa. Utilice la función plot classifier, dise~nada anteriormente, para construir gráficos de la solución en algunos casos representativos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t=19\n",
    "clf=Tree(criterion='gini',splitter='best',random_state=0,max_depth=n_t)\n",
    "clf.fit(X_train,Y_train)\n",
    "plot_classifier(clf,X_train,Y_train,X_test,Y_test,\"tree\",1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def DTC_plot(X_train,Y_train,X_test,Y_test):\n",
    "    \n",
    "    n_ts = range(2,21)\n",
    "    mse_DTC_train = []\n",
    "    mse_DTC_test = []\n",
    "\n",
    "    for n_t in n_ts:\n",
    "        clf=Tree(criterion='gini',splitter='best',random_state=0,max_depth=n_t)\n",
    "        clf.fit(X_train, Y_train)\n",
    "\n",
    "        y_pred_DTC_train = clf.predict(X_train)\n",
    "        y_pred_DTC_test = clf.predict(X_test)\n",
    "\n",
    "        mse_DTC_train.append(1-accuracy_score(Y_train, y_pred_DTC_train))\n",
    "        mse_DTC_test.append(1-accuracy_score(Y_test, y_pred_DTC_test))\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    DTC_plot = plt.subplot(1,1,1)\n",
    "    DTC_plot.plot(n_ts, mse_DTC_train, label='Training Misclassification Error')\n",
    "    DTC_plot.plot(n_ts, mse_DTC_test, label='Testing Misclassification Error')\n",
    "    plt.legend(bbox_to_anchor=(1,0.6))\n",
    "    plt.ylabel('Miss Classification Error')\n",
    "    plt.xlabel('Allows Levels')\n",
    "    plt.title(\"DTC\")\n",
    "    plt.show()\n",
    "    \n",
    "DTC_plot(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)** Como ya se demostró experimentalmente que este problema es linealmente inseperable, ahora se pide experimentar otra alternativa. Para ello deberá realizar una proyección de los datos a un nuevo espacio dimensional (manifold) en el cual se reconozcan sus patrones no lineales, para poder trabajarlos con fronteras lineales. Utilice la técnica de PCA con la ayuda de un Kernel Gaussiano  para extraer sus vectores con dimensión infinita de mayor varianza.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=2,kernel=\"rbf\", gamma=5)\n",
    "kpca = kpca.fit(X_train)\n",
    "Xkpca_train = kpca.transform(X_train)\n",
    "Xkpca_test = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    Analisis Cualitativo.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(g)** Ajuste un algoritmo de aprendizaje con fronteras lineal para los datos proyectados en este nuevo espacio que captura sus componentes no lineales, muestre graficamente que el problema ahora puede ser resulto con estos métodos. Reporte métricas para evaluar el desempeño, comente y concluya.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "Tree.fit(Xkpca_train, Y_train)\n",
    "plot_classifier(Tree,Xkpca_train,Y_train,Xkpca_test,Y_test,\"DTC\",0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(C=100, penalty='l2', tol=0.01)\n",
    "LR.fit(Xkpca_train, Y_train)\n",
    "plot_classifier(LR,Xkpca_train,Y_train,Xkpca_test,Y_test,\"LR\",0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVM = SVM(kernel='linear')\n",
    "model_SVM.fit(Xkpca_train,Y_train)\n",
    "plot_classifier(model_SVM,Xkpca_train,Y_train,Xkpca_test,Y_test,\"SVM\",0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Bike Sharing: Predicción de Demanda Horaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    En esta sección simularemos nuestra participación en el desafío Bike Sharing Demand de Kaggle. El objetivo es predecir la demanda de bicicletas sobre la red Capital Bikeshare de la ciudad de Washington, D.C., en función de la hora del día y otras variables descritas en la tabla 1. En principio, y como muestra la figura, la función es altamente no lineal y no determinista como función de la hora del día. Su objetivo será entrenar un modelo para obtener un puntaje correspondiente al top-100 del \"leaderboard\" final, es decir superior o igual a 0.37748. La función utilizada para evaluar este concurso Kaggle se proporciona en la siguiente ecuación:\n",
    "</p>\n",
    "\n",
    "$$\n",
    " E_{bikes}(y, \\hat{y}) = \\sqrt{\\frac{1}{n} \\sum_i (ln(y_i + 1) - ln(\\hat{y}_i+1))^2},\n",
    "$$\n",
    "<p  style=\"text-align: justify;\">\n",
    "donde $y, \\hat{y} \\in \\mathbb{R}^n$ denotan los vectores de observaciones y predicciones respectivamente. Correción de la fórmula desde: https://www.kaggle.com/c/bike-sharing-demand#evaluation\n",
    "<br><br>\n",
    "Como el dataset de pruebas original no está disponible se fabricará uno, correspondiente al 20% de los datos de entrenamiento. Además, se pondrá a su disposición un subconjunto independiente de datos con propósitos de validación.\n",
    "\n",
    "</p>\n",
    "\n",
    "<img src=\"grafico_robot.png\" style=\"height:300px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)** Cargue los datos de entrenamiento y pruebas como dataframes de _pandas_. Describa las variables involucradas en el problema, explorando el tipo de datos de que se trata, el número de valores distintos y, si corresponde, un gráfico (e.g. un histograma) que resuma su comportamiento. Su primera operación de pre-procesamiento de datos será obtener la hora del día desde el campo fecha (que en este momento es de tipo string), creando una nueva columna denominada hour y de tipo int. Para hacer esta operación se concatenarán los dataframes de entrenamiento y pruebas y luego se volverán a separar manteniendo la separación original.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dftrain = pd.read_csv('bike_sharing_train.csv')\n",
    "dfval = pd.read_csv('bike_sharing_val.csv')\n",
    "dftest = pd.read_csv('bike_sharing_test.csv')\n",
    "ntrain = len(dftrain)\n",
    "nval = len(dftrain) + len(dfval)\n",
    "df = pd.concat([dftrain,dfval,dftest])\n",
    "print('\\nSummary - dataframe completo:\\n')\n",
    "print(df.describe())\n",
    "df['hour'] = pd.to_datetime(df['datetime']).apply(lambda x: x.strftime('%H'))\n",
    "df['hour'] = pd.to_numeric(df['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for column in ['season','workingday','weather','temp', 'atemp', 'humidity', 'windspeed']:\n",
    "    plt.hist(df[column])\n",
    "    plt.title(column)\n",
    "    plt.ylabel(\"Bikes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)** Entrene un árbol de regresión para resolver el problema usando parámetros por defecto. Con este fin, construya una matriz $X_{train}$ de forma $n_{train} \\times d_1$ que contenga los datos de entrenamiento en sus filas, seleccionando las columnas que desee/pueda utilizar para el entrenamiento. Implemente además, la función de evaluación que hemos definido anteriormente para este problema. Evalúe el árbol de regresión ajustado a los datos de entrenamiento sobre el conjunto de entrenamiento y pruebas. Construya un gráfico que compare las predicciones con los valores reales. En este punto usted debiese tener un modelo con puntaje del orden de 0:59, lo que lo dejará más o menos en la posición 2140 de la competencia.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as Tree\n",
    "import matplotlib.pyplot as plt\n",
    "def eval_bikemodel(y_predict,y_true):\n",
    "    diff = np.log(y_predict+1.0) - np.log(y_true+1.0)\n",
    "    return np.sqrt(np.sum(np.square(diff))/len(y_predict))\n",
    "Xdf=df.loc[:,['season','holiday','workingday','weather','temp','atemp','humidity','windspeed','hour']]\n",
    "Ydf=df.loc[:,'count']\n",
    "X_train = Xdf[0:ntrain].values\n",
    "X_val = Xdf[ntrain:nval].values\n",
    "X_test = Xdf[nval:].values\n",
    "Y_train = Ydf[0:ntrain].values\n",
    "Y_val = Ydf[ntrain:nval].values\n",
    "Y_test = Ydf[nval:].values\n",
    "\n",
    "model = Tree(random_state=0)\n",
    "model.fit(X_train,Y_train)\n",
    "score_test = model.score(X_test,Y_test)\n",
    "print(\"SCORE TEST=%f\"%score_test)\n",
    "\n",
    "Y_pred_train = model.predict(X_train)\n",
    "Y_pred_val = model.predict(X_val)\n",
    "Y_pred_test = model.predict(X_test)\n",
    "kagg_train = eval_bikemodel(Y_pred_train,Y_train)\n",
    "kagg_val = eval_bikemodel(Y_pred_val,Y_val)\n",
    "kagg_test = eval_bikemodel(Y_pred_test,Y_test)\n",
    "\n",
    "print(\"KAGG EVAL TRAIN =%f\"%kagg_train)\n",
    "print(\"KAGG EVAL TEST =%f\"%kagg_test)\n",
    "plt.plot(Y_test,Y_pred_test,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    asdf\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)** Mejore el árbol de regresión definido en el punto anterior haciendo modificaciones a los hiper-parámetros del modelo. Por ejemplo, como estos modelos tienden a sobre-ajustar, podría intentar limitar la profundidad del árbol (¿Por qué esto debiese ayudar?). Naturalmente, está absolutamente prohibido tomar este tipo de decisiones en función del resultado de pruebas. Debe realizar estas elecciones evaluando sobre el conjunto de validación. Si no desea utilizarlo, y prefiere implementar validación cruzada u otra técnica automática, tiene la ventaja de poder usar el conjunto de validación como parte del entrenamiento. Con estas modificaciones debiese poder mejorar su ranking en unas 300 posiciones.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def train_tree(X,Y,max_depth,crit=[\"mse\",\"mae\"]):\n",
    "    scores = []\n",
    "    for d in range(1,max_depth):\n",
    "        for c in crit:\n",
    "            model = Tree(random_state=0,max_depth=d,criterion=c)\n",
    "            score_bike = cross_val_score(model, X,Y, scoring=make_scorer(eval_bikemodel)).mean()\n",
    "            scores.append([model,score_bike, c, d])\n",
    "    return scores\n",
    "\n",
    "scores = train_tree(np.concatenate([X_train,X_val]),np.concatenate([Y_train,Y_val]),30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 40\n",
    "depths = range(1,max_depth)\n",
    "kagg_vals =  []\n",
    "i_max = 1\n",
    "kagg_aux = 1\n",
    "for i in depths:\n",
    "    model = Tree(random_state=0,max_depth=i)\n",
    "    model.fit(X_train,Y_train)\n",
    "    #model.fit(X_val,Y_val)\n",
    "    Y_pred_val = model.predict(X_val)\n",
    "    kagg_val = eval_bikemodel(Y_pred_val,Y_val)\n",
    "    kagg_vals.append(kagg_val)\n",
    "    if kagg_aux > kagg_val:\n",
    "        kagg_aux = kagg_val\n",
    "        i_max = i\n",
    "plt.plot(depths,kagg_vals)\n",
    "plt.xlabel(\"Depths\")\n",
    "plt.ylabel(\"Kagg Values\")\n",
    "plt.title(\"DRT validation set\")\n",
    "plt.show()\n",
    "print(\"KAGG EVAL VAL =%f\"%kagg_vals[i_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_score_cv(scores):\n",
    "    sorted_scores = sorted(scores, key=(lambda x: x[1]), reverse=False)\n",
    "    print(\"Criterion:\",sorted_scores[0][2])\n",
    "    print(\"Max Depth: %f\"%sorted_scores[0][3])\n",
    "    print(\"KAGG EVAL VAL =%f\"%sorted_scores[0][1])\n",
    "    return sorted_scores\n",
    "bs = best_score_cv(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "  asdf\n",
    "  \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)**  Mejore el árbol de regresión definido en el punto anterior haciendo modificaciones sobre la representación utilizada para aprender desde los datos. Por ejemplo, los histogramas que construyó en el punto (a) así como la forma especial de la función de evaluación, sugieren una cierta transformación de la variable respuesta. Podría intentar también normalizando los datos o normalizando la respuesta. Otra opción es intentar rescatar algo más acerca de la fecha (anteriormente sólo se extrajo la hora), como por ejemplo el año o el día de la semana ('lunes','martes', etc) que corresponde. Sea creativo, este paso le debiese reportar un salto de calidad muy significativo. Una observación importante es que si hace una transformación a la variable respuesta (por ejemplo raíz cuadrada), debe invertir esta transformación antes de evaluar el desempeño con *eval_bikemodel* (por ejemplo, elevar al cuadrado si tomó raíz cuadrada). Con modificaciones de este tipo, podría mejorar su ranking en unas 1000 posiciones, entrando ya al top-1000 con un score del orden de $0.45$.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cday'] = pd.to_datetime(df['datetime']).dt.dayofweek #0:Lunes,6:Domingo\n",
    "df['cday'] = pd.to_numeric(df['cday'])\n",
    "df['cyear'] = pd.to_datetime(df['datetime']).dt.year #Año\n",
    "df['cyear'] = pd.to_numeric(df['cyear'])\n",
    "df['cmonth'] = pd.to_datetime(df['datetime']).dt.month #1:Enero,12:Diciembre\n",
    "df['cmonth'] = pd.to_numeric(df['cmonth'])\n",
    "df['holiday'] = pd.to_numeric(df['holiday'])\n",
    "df['workingday'] = pd.to_numeric(df['workingday'])\n",
    "df['tday'] = df['holiday'] + 2*df['workingday']\n",
    "Xdf=df.loc[:,['season','weather','temp','atemp','humidity','windspeed','hour','tday','cday','cmonth','cyear']]\n",
    "Xdf\n",
    "#Xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xdf = (Xdf - Xdf.mean()) / (Xdf.max() - Xdf.min())\n",
    "#Ydf = (Ydf - Ydf.mean()) / (Ydf.max() - Ydf.min())\n",
    "X_train = Xdf[0:ntrain].values\n",
    "X_val = Xdf[ntrain:nval].values\n",
    "X_test = Xdf[nval:].values\n",
    "Y_train = Ydf[0:ntrain].values\n",
    "Y_val = Ydf[ntrain:nval].values\n",
    "Y_test = Ydf[nval:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 40\n",
    "depths = range(1,max_depth)\n",
    "kagg_vals =  []\n",
    "i_max = 1\n",
    "kagg_aux = 1\n",
    "for i in depths:\n",
    "    model = Tree(random_state=0,max_depth=i)\n",
    "    model.fit(X_train,Y_train)\n",
    "    #model.fit(X_val,Y_val)\n",
    "    Y_pred_val = model.predict(X_val)\n",
    "    kagg_val = eval_bikemodel(Y_pred_val,Y_val)\n",
    "    kagg_vals.append(kagg_val)\n",
    "    if kagg_aux > kagg_val:\n",
    "        kagg_aux = kagg_val\n",
    "        i_max = i\n",
    "plt.plot(depths,kagg_vals)\n",
    "plt.xlabel(\"Depths\")\n",
    "plt.ylabel(\"Kagg Values\")\n",
    "plt.title(\"DRT validation set\")\n",
    "plt.show()\n",
    "print(\"KAGG EVAL VAL =%f\"%kagg_vals[i_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_tree(np.concatenate([X_train,X_val]),np.concatenate([Y_train,Y_val]),30)\n",
    "bs = best_score_cv(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs[0][0].fit(X_train,Y_train)\n",
    "print(\"Top BikeScore Test: %f\"%eval_bikemodel(bs[0][0].predict(X_test),Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)** Entrene una SVM no lineal para resolver el problema midiendo el efecto de las distintas representaciones que haya descubierto hasta este punto. Un detalle importante es que antes de entrenar la SVM sería aconsejable hacer dos tipos de pre-procesamiento adicional de los datos: (i) codificar las variables categóricas en un modo apropiado - por ejemplo como vector binario con un 1 en la posición del valor adoptado-, (ii) escalar los atributos de modo que queden centrados y con rangos comparables. Usando parámetros por defecto para la SVM debiese obtener un score del orden de $0.344$, quedando definitivamente en el top-10 de la competencia.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframes as before ...\n",
    "df = pd.concat([dftrain,dfval,dftest])\n",
    "df['hour'] = pd.to_datetime(df['datetime']).apply(lambda x: x.strftime('%H'))\n",
    "df['hour'] = pd.to_numeric(df['hour'])\n",
    "\n",
    "df['cday'] = pd.to_datetime(df['datetime']).dt.dayofweek #0:Lunes,6:Domingo\n",
    "df['cday'] = pd.to_numeric(df['cday'])\n",
    "df['cyear'] = pd.to_datetime(df['datetime']).dt.year #Año\n",
    "df['cyear'] = pd.to_numeric(df['cyear'])\n",
    "df['cmonth'] = pd.to_datetime(df['datetime']).dt.month #1:Enero,12:Diciembre\n",
    "df['cmonth'] = pd.to_numeric(df['cmonth'])\n",
    "\n",
    "Xdf=df.loc[:,['season','holiday','workingday','weather','temp','atemp','humidity','windspeed','hour','cyear','cmonth','cday']]\n",
    "\n",
    "#PASO IMPORTANTE MAS ABAJO ...\n",
    "Xdf = pd.get_dummies(Xdf,columns=['season', 'weather','hour','cday'])\n",
    "\n",
    "#normalización + 1 para eliminar los 0 del logaritmo\n",
    "Ydf = np.log(df.loc[:,'count']+1)\n",
    "X_train = Xdf[0:ntrain].values\n",
    "X_val = Xdf[ntrain:nval].values\n",
    "X_test = Xdf[nval:].values\n",
    "Y_train = Ydf[0:ntrain].values\n",
    "Y_val = Ydf[ntrain:nval].values\n",
    "Y_test = Ydf[nval:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerX = StandardScaler()\n",
    "X_train = scalerX.fit_transform(X_train)\n",
    "X_val = scalerX.fit_transform(X_val)\n",
    "X_test = scalerX.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "model = SVR()\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred_train = model.predict(X_train)\n",
    "Y_pred_val = model.predict(X_val)\n",
    "Y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test = model.score(X_test,Y_pred_test)\n",
    "print(\"SCORE TEST=%f\"%score_test)\n",
    "kagg_train = eval_bikemodel(np.exp(Y_pred_train)-1,np.exp(Y_train)-1)\n",
    "kagg_val = eval_bikemodel(np.exp(Y_pred_val)-1,np.exp(Y_val)-1)\n",
    "kagg_test = eval_bikemodel(np.exp(Y_pred_test)-1,np.exp(Y_test)-1)\n",
    "print(\"KAGG EVAL TRAIN =%f\"%kagg_train)\n",
    "print(\"KAGG EVAL VAL =%f\"%kagg_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "   asdf\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)** Mejore la SVM definida en el punto anterior haciendo modificaciones a los hiper-parámetros de la máquina ($C$, $\\epsilon$ o la misma función de kernel). Naturalmente, está absolutamente prohibido tomar este tipo de decisiones de diseño mirando el resultado de pruebas. Debe realizar estas elecciones evaluando sobre el conjunto de validación. Si no desea utilizarlo, y prefiere implementar validación cruzada u otra técnica automática, tiene la ventaja de poder usar el conjunto de validación como parte del entrenamiento.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "rango = range(1,np.linalg.matrix_rank(X_val))\n",
    "kagg_vals= []\n",
    "for i in rango:\n",
    "    model = SVR(C=i,epsilon=0.01)\n",
    "    model.fit(X_train,Y_train)\n",
    "    Y_pred_val = model.predict(X_val)\n",
    "    kagg_val = eval_bikemodel(np.exp(Y_pred_val)-1,np.exp(Y_val)-1)\n",
    "    kagg_vals.append(kagg_val)\n",
    "plt.plot(rango,kagg_vals)\n",
    "plt.show()\n",
    "#kagg_train = eval_bikemodel(Y_pred_train,Y_train)\n",
    "#kagg_val = eval_bikemodel(Y_pred_val,Y_val)\n",
    "#print(\"KAGG EVAL TRAIN =%f\"%kagg_train)\n",
    "#print(\"KAGG EVAL VAL =%f\"%kagg_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(g)**  Evaúe el efecto de utilizar el dataset de validación para entrenamiento y seleccionar los parámetros estructurales del árbol de clasificación y la SVM usando validación cruzada. El código de ejemplo para esto ha sido proporcionado en las tareas 1 y 2, pero se adjunta de nuevo a continuación\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "def eval_bikemodel_log(y_predict,y_true,**kwargs):\n",
    "    y_predict = np.exp(y_predict)-1\n",
    "    y_true    = np.exp(y_true)-1\n",
    "    diff = np.log(y_predict+1.0) - np.log(y_true+1.0)\n",
    "    return np.sqrt(np.sum(np.square(diff))/len(y_predict))\n",
    "\n",
    "def train_svm(X_train,Y_train,max_c=20,min_c=1):\n",
    "    scores = []\n",
    "    c_range = range(min_c,max_c+1)\n",
    "    e_range = [0.01,0.05,0.1,0.5,1,10]\n",
    "    for c in c_range:\n",
    "        for e in e_range:\n",
    "            model = SVR(C=c,epsilon=e,kernel='rbf')\n",
    "            score_bike = cross_val_score(model, X_train,Y_train, scoring=make_scorer(eval_bikemodel_log)).mean()\n",
    "            scores.append([model,score_bike, c, e])\n",
    "    return scores\n",
    "scores = train_svm(np.concatenate([X_train,X_val]),np.concatenate([Y_train,Y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = best_score_cv(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "asdfs\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(h)**  Evalúe el efecto de utilizar un ensamblado de 2 máquinas de aprendizaje para predecir la demanda total de bicicletas. Un modelo se especializará en la predicción de la demanda de bicicletas de parte de usuarios registrados y otra en la predicción de la demanda de usuarios casuales. Hay razones claras para pensar que los patrones son distintos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ydf=df.ix[:,'count'] #demanda total\n",
    "Ydf=df.ix[:,'registered'] #demanda registrada\n",
    "Ydf=df.ix[:,'casual'] #demanda casual\n",
    "\n",
    "Ydf_r=np.log(df['registered']+1) #demanda registrada\n",
    "Yr_train = Ydf_r[0:ntrain].values\n",
    "Yr_val = Ydf_r[ntrain:nval].values\n",
    "Yr_test = Ydf_r[nval:].values\n",
    "\n",
    "Ydf_c=np.log(df['casual']+1) #demanda casual\n",
    "Yc_train = Ydf_c[0:ntrain].values\n",
    "Yc_val = Ydf_c[ntrain:nval].values\n",
    "Yc_test = Ydf_c[nval:].values\n",
    "\n",
    "model_r = SVR(C=t[3],kernel=t[4],epsilon=t[5])\n",
    "model_c = SVR(C=t[3],kernel=t[4],epsilon=t[5])\n",
    "\n",
    "model_r.fit(np.concatenate([X_train,X_val]),np.concatenate([Yr_train,Yr_val]))\n",
    "model_c.fit(np.concatenate([X_train,X_val]),np.concatenate([Yc_train,Yc_val]))\n",
    "\n",
    "Yr_pred_test = model_r.predict(X_test)\n",
    "Yc_pred_test = model_c.predict(X_test)\n",
    "\n",
    "print(\"Top BikeScore Test registered: %f\"%eval_bikemodel_log(Yr_pred_test,Yr_test))\n",
    "print(\"Top BikeScore Test casual    : %f\"%eval_bikemodel_log(Yc_pred_test,Yc_test))\n",
    "print(\"Top BikeScore Test count     : %f\"%eval_bikemodel_log(Yr_pred_test+Yc_pred_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(i)**  Evalúe el efecto de utilizar un algoritmo genérico para ensamblar máquinas de aprendizaje para predecir la demanda total de bicicletas. Puede experimentar con una sola técnica (e.g. Random Forest), discuta la evolución a medida que aumenta el número de máquinas.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def train_random_forest(X_train,Y_train,X_val,Y_val, estimators, depth):\n",
    "    model = RandomForestRegressor(n_estimators=estimators,max_depth=depth, random_state=0)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    return (model,eval_bikemodel_log(y_val_pred,y_val),estimators,depth)\n",
    "\n",
    "def best_scores_rfr(scores):\n",
    "    sorted_scores = sorted(scores, key=(lambda x: x[1]), reverse=False)\n",
    "    return sorted_scores[0]\n",
    "\n",
    "scores = []\n",
    "count,current=20*20,1\n",
    "for e in range(1,20):\n",
    "    for d in range(1,20):\n",
    "        scores.append(train_random_forest(X_train,Y_train,X_val,Y_val, e, d))\n",
    "        print (\"%d/%d\"%(current,count), end=\"\\r\")\n",
    "        current+=1\n",
    "        \n",
    "t=best_scores_rfr(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit(X_train,Y_train)\n",
    "y_test_pred = t.predict(X_test)\n",
    "\n",
    "print(\"Top BikeScore Test: %f\"%eval_bikemodel_log(y_test_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Calidad de un vino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "Dentro de las muchas variedades de vino existentes, algunas gustan más que otras, esto es debido al gusto de una persona en particular o bien a la gran cantidad de químicos y procesos que se aplican a la producción de vino. Para el área de negocios, el estimar cuál es la calidad de un vino en base a la apreciación del público es una tarea bastante difícil. <br>\n",
    "Para esta actividad se trabajará con dos datasets asociados a las variantes tinto y blanco del vino portugués \"Vinho Verde\"[4]. Debido a temas privados solo se cuenta con las característcas fisioquímicas asociadas a un vino en particular, los cuales corresponden a 11 atributos numéricos descritos en el siguiente link. Este problema puede ser abordado como clasificación de 11 clases o de regresión, ya que el atributo a estimar, _quality_, es un valor entero entre 0 y 10.\n",
    "</p>\n",
    "\n",
    "Descripción: http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)**  Carge los dos dataset en un único dataframe de pandas, además de agregar una columna indicando si es vino tinto o blanco. Describa el dataset a trabajar.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_red = pd.read_csv(\"winequality-red.csv\",sep=\";\")\n",
    "df_red['type'] = 'red'\n",
    "\n",
    "df_white = pd.read_csv(\"winequality-white.csv\",sep=\";\")\n",
    "df_white['type'] = 'white'\n",
    "\n",
    "df = pd.concat([df_red,df_white], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\">\n",
    "    El nuevo dataframe contiene 1599 datos sobre vino tinto, y 4898 sobre vino blanco. Cada uno de estos datos posee 12 atributos. En la descripción del dataset se nos advierte que mucho de estos atributos pueden estar correlacionados por lo tanto, tiene sentido aplicar algún tipo de selección de atributos. <br>\n",
    "</p>\n",
    "\n",
    "<ol>\n",
    "<li> **fixed acidity** - la mayoría de los ácidos involucrados con el vino son fijos o no volátiles (no se evaporan fácilmente).</li>\n",
    "<li> **volatile acidity** - la cantidad de ácido acético en el vino, que a niveles demasiado altos puede conducir a un desagradable sabor a vinagre.</li>\n",
    "<li> **citric acid** - encontrado en pequeñas cantidades, el ácido cítrico puede agregar 'frescura' y sabor a los vinos. </li>\n",
    "<li> **residual sugar** - la cantidad de azúcar que queda después de que la fermentación se detiene, es raro encontrar vinos con menos de 1 gramo / litro y los vinos con más de 45 gramos / litro se consideran dulces.</li>\n",
    "<li> **chlorides** - la cantidad de sal en el vino. </li>\n",
    "<li> **free sulfur dioxide** - la forma libre de SO2 existe en equilibrio entre el SO2 molecular (como gas disuelto) y el ion bisulfito; previene el crecimiento microbiano y la oxidación del vino.</li>\n",
    "<li> **total sulfur dioxide** - cantidad de S02 en forma libre y unida; en bajas concentraciones, el SO2 es casi indetectable en el vino, pero a concentraciones superiores a 50 ppm, el SO2 se hace evidente en la nariz y el sabor del vino. </li>\n",
    "<li> **density** - la densidad del vino es cercana a la del agua según el porcentaje de contenido de alcohol y azúcar.</li>\n",
    "<li> **pH** - describe cuán ácido o básico es un vino en una escala de 0 (muy ácida) a 14 (muy básica); la mayoría de los vinos están entre 3-4 en la escala de pH.</li>\n",
    "<li> **sulphates** - un aditivo de vino que puede contribuir a los niveles de gas de dióxido de azufre (S02), que actúa como un antimicrobiano y antioxidante.</li>\n",
    "<li> **alcohol** - el porcentaje de contenido de alcohol del vino. </li>\n",
    "<li> **quality** - calidad del vino (puntaje entre 0 y 10). </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)**  Aborde este problema como si fuera de clasificación binaria para predecir si un vino es de buena calidad o no, es decir, utilice las distintas características fisioquímicas presentes en los datos para estimar esta etiqueta. Para esto cree las matrices de entrenamiento y de pruebas, además de la etiqueta para ambos conjuntos, considerando como quality mayor a 5 un vino de buena calidad. El conjunto de pruebas (25%) será utilizado únicamente para verificar la calidad de los algoritmos a entrenar.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df['good_quality'] = [1 if q>5 else 0 for q in df.quality]\n",
    "\n",
    "df.dropna(axis=0)\n",
    "\n",
    "X = df.iloc[:,0:11] #todos los atributos\n",
    "y = df.good_quality.values # Variable binaria \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)**  Entrene un solo Árbol de Clasificación de múltiples niveles para resolver el problema. Puede variar los hiper-parámetros que prefiera, recuerde que las decisiones no pueden ser basadas mirando el conjunto de pruebas. Debido al desbalanceo que se produce en las dos clases mida la métrica F1-_score_ sobre el conjunto de entrenamiento y de pruebas.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf=Tree(criterion='gini', splitter='best', random_state=0, max_depth=5)\n",
    "clf.fit(X_train,y_train)\n",
    "acc_test = clf.score(X_test,y_test)\n",
    "print(\"Test Accuracy = %f\"%acc_test)\n",
    "print(clf.tree_.max_depth)\n",
    "#plot_classifier(clf,X_train,Y_train,X_test,Y_test,'tree',1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DTC_plot(X_train , y_train,X_test, y_test):\n",
    "\n",
    "    Depths = range(1,30)\n",
    "\n",
    "    F1Score_Train = []\n",
    "    F1Score_Test = []\n",
    "\n",
    "    for i in Depths:\n",
    "        model_DTC = Tree(max_depth = i)\n",
    "\n",
    "        model_DTC.fit(X_train , y_train)\n",
    "\n",
    "        y_pred_DTC_train = model_DTC.predict(X_train )\n",
    "        y_pred_DTC_test = model_DTC.predict(X_test)\n",
    "\n",
    "        F1Score_Train.append(f1_score(y_train, y_pred_DTC_train))\n",
    "        F1Score_Test.append(f1_score(y_test, y_pred_DTC_test))\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    DTC_plot = plt.subplot(1,1,1)\n",
    "    DTC_plot.plot(Depths, F1Score_Train, label='Training F1 Score')\n",
    "    DTC_plot.plot(Depths, F1Score_Test, label='Testing F1 Score')\n",
    "    plt.legend(loc=3)\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xlabel('Depth DTC')\n",
    "    plt.axis([0,30,0.0, 1.0])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "DTC_plot(X_train , y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"F1-Score Train: %f\" %f1_score(y_train, clf.predict(X_train)))\n",
    "print(\"F1-Score Test: %f\" %f1_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)**  Entrene un ensamblador de árboles de múltiples niveles, mediante la técnica de _Random Forest_. Varíe la cantidad de árboles de decisión utilizados en el ensamblado (*n_estimators*), realice un gráfico resumen del F1-_score_ de entrenamiento y de pruebas en función de este hiper-parámetro.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1Score_plot(model, X_train , y_train,X_test, y_test):\n",
    "\n",
    "    Estimators = range(1,100)\n",
    "\n",
    "    F1Score_Train = []\n",
    "    F1Score_Test = []\n",
    "\n",
    "    for i in Estimators:\n",
    "        if (model == RandomForestClassifier):\n",
    "            model_DTC = model(n_estimators = i, max_depth = 20, n_jobs =-1)\n",
    "        else:\n",
    "            model_DTC = model(base_estimator=Tree(max_depth=20), n_estimators = i)\n",
    "\n",
    "        model_DTC.fit(X_train , y_train)\n",
    "        y_pred_DTC_train = model_DTC.predict(X_train )\n",
    "        y_pred_DTC_test = model_DTC.predict(X_test)\n",
    "\n",
    "        F1Score_Train.append(f1_score(y_train, y_pred_DTC_train))\n",
    "        F1Score_Test.append(f1_score(y_test, y_pred_DTC_test))\n",
    "\n",
    "    plt.figure(figsize=(18,12))\n",
    "\n",
    "    RF_DTC_plot = plt.subplot(1,1,1)\n",
    "    RF_DTC_plot.plot(Estimators, F1Score_Train, label='Training F1 Score')\n",
    "    RF_DTC_plot.plot(Estimators, F1Score_Test, label='Testing F1 Score')\n",
    "    plt.legend(loc=3)\n",
    "    plt.ylabel('F1 Score', fontsize=18)\n",
    "    plt.xlabel('Number of Estimators', fontsize=18)\n",
    "    plt.axis([0, 100,0.0, 1.0])\n",
    "    \n",
    "    ind_max = np.argmax(F1Score_Test)\n",
    "    print(\"F1 Score %f con %d estimadores\" %(F1Score_Test[ind_max], ind_max))\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "F1Score_plot(RandomForestClassifier, X_train , Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)**  Entrene un ensamblador de árboles de múltiples niveles, mediante la técnica de _AdaBoost_. Varíe la cantidad de árboles de decisión utilizados en el ensamblado (*n_estimators*), realice un gráfico resumen del F1-_score_ de entrenamiento y de pruebas en función de este hiper-parámetro. Compare y analice con la técnica utilizada en d).\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "\n",
    "F1Score_plot(ABC, X_train , Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)** Entrene alguna otra máquina de aprendizaje, elegida por usted, para resolver este problema. Elija los hiper-parámetros que estime convenientes intentando aumentar el F1-score obtenido por los algoritmos anteriores. Compare y analice estas 4 maneras de resolver el problema definido en b).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=0)\n",
    "clf.fit(X_train, y_train) \n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "print(f1_score(y_train, y_train_pred))\n",
    "print(f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "pca_model = PCA(n_components=11)\n",
    "pca_model.fit(X_train,y_train)\n",
    "X_pca_train = pca_model.transform(X_train)\n",
    "X_pca_test = pca_model.transform(X_test)\n",
    "lda_model = LDA()\n",
    "lda_model.fit(X_train,y_train)\n",
    "X_lda_train = lda_model.transform(X_train)\n",
    "X_lda_test = lda_model.transform(X_test)\n",
    "\n",
    "model = LR()\n",
    "model.fit(X_pca_train,y_train)\n",
    "y_pca_pred_train = model.predict(X_pca_train)\n",
    "y_pca_pred_test = model.predict(X_pca_test)\n",
    "model.fit(X_lda_train,y_train)\n",
    "y_lda_pred_train = model.predict(X_lda_train)\n",
    "y_lda_pred_test = model.predict(X_lda_test)\n",
    "\n",
    "model.fit(X_pca_train,y_train)\n",
    "sy_pca_pred_train = model.predict(X_pca_train)\n",
    "sy_pca_pred_test = model.predict(X_pca_test)\n",
    "model.fit(X_lda_train,y_train)\n",
    "sy_lda_pred_train = model.predict(X_lda_train)\n",
    "sy_lda_pred_test = model.predict(X_lda_test)\n",
    "\n",
    "print(\"Train PCA + LR f1 score: %f\"%f1_score(y_train, y_pca_pred_train))\n",
    "print(\"Test PCA + LR f1 score: %f\"%f1_score(y_test, y_pca_pred_test))\n",
    "print(\"---------------------------\")\n",
    "print(\"Train LDA + LR f1 score: %f\"%f1_score(y_train, y_lda_pred_train))\n",
    "print(\"Test LDA + LR f1 score: %f\"%f1_score(y_test, y_lda_pred_test))\n",
    "print(\"---------------------------\")\n",
    "print(\"Train LDA + SVR f1 score: %f\"%f1_score(y_train, sy_lda_pred_train.round()))\n",
    "print(\"Test LDA + SVR f1 score: %f\"%f1_score(y_test, sy_lda_pred_test.round()))\n",
    "print(\"---------------------------\")\n",
    "print(\"Train PCA + SVR f1 score: %f\"%f1_score(y_train, sy_pca_pred_train.round()))\n",
    "print(\"Test PCA + SVR f1 score: %f\"%f1_score(y_test, sy_pca_pred_test.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(g)**  Defina un criterio para estimar la importancia de los distintos atributos en el ensamblado de _Random Forest_, implementelo sobre alguno de los ensambladores entrenados en d), haga un ranking de importancia de atributos ¿Es posible implementar este criterio sobre una técnica de _boost_ como lo es _AdaBoost_?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 84, max_depth = 20, n_jobs =-1)\n",
    "model.fit(X_train , y_train)\n",
    "\n",
    "#Return the feature importances (the higher, the more important the feature)\n",
    "w_RFC = model.feature_importances_\n",
    "\n",
    "model = ABC(base_estimator=Tree(max_depth=20), n_estimators = 65)\n",
    "model.fit(X_train , y_train)\n",
    "\n",
    "w_ABC = model.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_attributes = [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"]\n",
    "attributes = range(1,12)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "weights_plot = plt.subplot(1,1,1)\n",
    "weights_plot.plot(attributes, w_RFC, label='Weight of attributes RFC')\n",
    "weights_plot.plot(attributes, w_ABC, label='Weight of attributes ABC')\n",
    "plt.legend(loc=1)\n",
    "plt.ylabel('Weight', fontsize=18)\n",
    "#plt.xlabel('Attributes', fontsize=18)\n",
    "plt.axis([0, 12,0.0, .3])\n",
    "plt.xticks(attributes, str_attributes, size='large', rotation=80)\n",
    "\n",
    "for label in weights_plot.get_xmajorticklabels():\n",
    "    label.set_rotation(35)\n",
    "    label.set_horizontalalignment(\"right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    Es posible implementarlo pero sería redundar, ya que Adaboost, al contrario que las redes neuronales y las SVM, selecciona solo los atributos que dan pontencial información al proceso de predicción, por lo que no solo disminuye la dimensionalidad del proceso, sino que también disminuye el tiempo de computo de predicción despreciando los atributos que no aportan información alguna. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Reconocimiento de Imágenes Sign Gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    MNIST es un dataset muy popular de dígitos escrito a mano que a servido para probar distintos algoritmos de Machine Learning relacionados con Computer Vision. Buscando nuevos desafíos, investigadores generaron un dataset que podría usarse eventualmente en aplicaciones reales, Sign Gestures, consta de imagenes del lenguaje de señas, estas tienen una resolución de 28x28 pixeles representados en una escala de grises 0-255. La versión utilizada se atribuye a [8] y viene separada en 27455 ejemplos de entrenamiento y 7172 casos de pruebas. Las clases son mutuamente excluyentes y corresponden a las letras del alfabeto (ver imagen).\n",
    "</p>\n",
    "<img src=\"Gestos.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(a)**  Construya una función que cargue todos los datos de entrenamiento y pruebas del problema generando como salida: (i) dos matrices $X_{tr}, Y_{tr}$, correspondientes a las imágenes y etiquetas de entrenamiento, (ii) dos matrices  $X_{t}, Y_{t}$, correspondientes a las imágenes y etiquetas de pruebas, y finalmente (iii) dos matrices  $X_{v}, Y_{v}$ correspondientes a imágenes y etiquetas que se usarán como conjunto de validación, es decir para tomar decisiones de diseño acerca del modelo. Este último conjunto debe ser extraído desde el conjunto de entrenamiento original y no debe superar las 7000 imágenes.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_data():\n",
    "    train = pd.read_csv('sign_mnist_train.csv')\n",
    "    test = pd.read_csv('sign_mnist_test.csv')\n",
    "    y_tr = train['label']\n",
    "    x_tr = train.iloc[:,1:]\n",
    "    y_t = test['label']\n",
    "    x_t = test.iloc[:,1:]\n",
    "    x_tr, x_v, y_tr, y_v = train_test_split(x_tr, y_tr, test_size=0.25, random_state=1) \n",
    "    return(x_tr,x_v,x_t,y_tr,y_v,y_t)\n",
    "\n",
    "x_tr, x_v, x_t, y_tr, y_v , y_t = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(b)**  Construya una función que escale apropiadamente las imágenes antes de trabajar. Experimente sólo escalando los datos de acuerdo a la intensidad máxima de pixel (i.e., dividiendo por 255) y luego centrando y escalándolos como en actividades anteriores.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def image_scale(X_tr,X_v,X_t):\n",
    "    X_tr=X_tr/255\n",
    "    X_v=X_v/255\n",
    "    X_t=X_t/255\n",
    "    scalerX = StandardScaler()\n",
    "    X_train = scalerX.fit_transform(X_tr)\n",
    "    X_val = scalerX.fit_transform(X_v)\n",
    "    X_test = scalerX.transform(X_t)\n",
    "    return X_train,X_val,X_test\n",
    "X_train,X_val,X_test = image_scale(x_tr,x_v,x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(c)**  Diseñe, entrene y evalúe una red neuronal para el problema partir de la representación original de las imágenes. Experimente con distintas arquitecturas, pre-procesamientos y métodos de entrenamiento, midiendo el error de clasificación sobre el conjunto de validación. En base a esta última medida de desempeño, decida qué modelo, de entre todos los evaluados, medirá finalmente en el conjunto de test. Reporte y discuta los resultados obtenidos. Se espera que logre obtener un error de pruebas menor o igual a 0.2.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, \n",
    "          batch_size=128, verbose=1,validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(d)**  Para la mejor red entrenada anteriormente construya la matriz de confusión de las distintas clases, para asi visualizar cuáles son las clases más difíciles de clasificar y con cuáles se confunden. Comente.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\"]\n",
    "matrix = confusion_matrix(y_true, y_pred,labels=labels)\n",
    "df = pd.DataFrame(data=matrix,columns=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(e)**  Entrene una SVM no lineal sobre los pixeles con y sin pre-procesamiento. Puede utilizar el conjunto de validación para seleccionar hiper-parámetros, como el nivel de regularización aplicado y/o la función de kernel a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model.fit(x_tr,y_tr)\n",
    "y_pred_train = model.predict(x_tr)\n",
    "y_pred_val = model.predict(x_v)\n",
    "y_pred_test = model.predict(x_t)\n",
    "model.fit(X_train,y_tr)\n",
    "y_pred_train2 = model.predict(X_train)\n",
    "y_pred_val2 = model.predict(X_val)\n",
    "y_pred_test2 = model.predict(X_test)\n",
    "print(\"Accuracy sin pre procesamiento\")\n",
    "print(\"SVR accuracy: %f\"%(1-accuracy_score(y_tr, y_pred_train)))\n",
    "print(\"SVR accuracy: %f\"%(1-accuracy_score(y_v, y_pred_val)))\n",
    "print(\"SVR accuracy: %f\"%(1-accuracy_score(y_t, y_pred_test)))\n",
    "print(\"Accuracy con pre procesamiento\")\n",
    "print(\"SVR accuracy: %f\"%(1-accuracy_score(y_tr, y_pred_train2)))\n",
    "print(\"SVR accuracy: %f\"%(1-accuracy_score(y_v, y_pred_val2)))\n",
    "print(\"SVR accuracy: %f\"%(1-accuracy_score(y_t, y_pred_test2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p  style=\"text-align: justify;\"> \n",
    "    **(f)** Entrene una árbol de clasificación sobre los pixeles con y sin pre-procesamiento. Puede utilizar el conjunto de validación para seleccionar hiper-parámetros, como la profundidad máxima del árbol.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "model = Tree() #edit the train_model function\n",
    "max_depths = range(1,30)\n",
    "acc_train = []\n",
    "acc_val = []\n",
    "acc_test = []\n",
    "pre_acc_train = []\n",
    "pre_acc_val = []\n",
    "pre_acc_test = []\n",
    "for i in max_depths:\n",
    "    model.set_params(max_depth=i,criterion='entropy',splitter='best')\n",
    "    model.fit(x_tr,y_tr)\n",
    "    y_pred_train = model.predict(x_tr)\n",
    "    y_pred_val = model.predict(x_v)\n",
    "    y_pred_test = model.predict(x_t)\n",
    "    model.fit(X_train,y_tr)\n",
    "    y_pred_train2 = model.predict(X_train)\n",
    "    y_pred_val2 = model.predict(X_val)\n",
    "    y_pred_test2 = model.predict(X_test)\n",
    "    acc_train.append(accuracy_score(y_tr,y_pred_train)*100)\n",
    "    acc_val.append(accuracy_score(y_v,y_pred_val)*100)\n",
    "    acc_test.append(accuracy_score(y_t,y_pred_test)*100)\n",
    "    pre_acc_train.append(accuracy_score(y_tr,y_pred_train2)*100)\n",
    "    pre_acc_val.append(accuracy_score(y_v,y_pred_val2)*100)\n",
    "    pre_acc_test.append(accuracy_score(y_t,y_pred_test2)*100)\n",
    "plt.plot(max_depths,acc_train)\n",
    "plt.plot(max_depths,acc_val)\n",
    "plt.plot(max_depths,acc_test)\n",
    "plt.plot(max_depths,pre_acc_train)\n",
    "plt.plot(max_depths,pre_acc_val)\n",
    "plt.plot(max_depths,pre_acc_test)\n",
    "plt.show()\n",
    "#print(\"Accuracy sin pre procesamiento\")\n",
    "#print (\"Train Accuracy is : {0:.1f}%\".format(accuracy_score(y_tr,y_pred_train)*100))\n",
    "#print (\"Val Accuracy is : {0:.1f}%\".format(accuracy_score(y_v,y_pred_val)*100))\n",
    "#print (\"Test Accuracy is : {0:.1f}%\".format(accuracy_score(y_t,y_pred_test)*100))\n",
    "#print(\"Accuracy con pre procesamiento\")\n",
    "#print (\"Train Accuracy is : {0:.1f}%\".format(accuracy_score(y_tr,y_pred_train2)*100))\n",
    "#print (\"Val Accuracy is : {0:.1f}%\".format(accuracy_score(y_v,y_pred_val2)*100))\n",
    "#print (\"Test Accuracy is : {0:.1f}%\".format(accuracy_score(y_t,y_pred_test2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
